{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3437961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b667345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('LaunchData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "96f07167",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['RocketName'])\n",
    "x = df.drop(columns=['AltitudeHeight', 'TimeAtAltitude', 'TotalFlightTime'])\n",
    "y = df[['AltitudeHeight', 'TimeAtAltitude', 'TotalFlightTime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f1a327da",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.get_dummies(x, columns=['NoseType', 'FinShape'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "eccf5348",
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xVal, yTrain, yVal = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fc6e5457",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(xTrain.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(3)  \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "24cc39a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "12/12 - 1s - loss: 932990538972766666752.0000 - mae: 1684590336.0000 - val_loss: 60406844852315246034944.0000 - val_mae: 20984768512.0000 - 606ms/epoch - 50ms/step\n",
      "Epoch 2/200\n",
      "12/12 - 0s - loss: 932988005697976270848.0000 - mae: 1684727168.0000 - val_loss: 60406808823518227070976.0000 - val_mae: 20984840192.0000 - 60ms/epoch - 5ms/step\n",
      "Epoch 3/200\n",
      "12/12 - 0s - loss: 932985402054441697280.0000 - mae: 1684847744.0000 - val_loss: 60406772794721208107008.0000 - val_mae: 20984944640.0000 - 56ms/epoch - 5ms/step\n",
      "Epoch 4/200\n",
      "12/12 - 0s - loss: 932983713204581433344.0000 - mae: 1685046528.0000 - val_loss: 60406727758724934402048.0000 - val_mae: 20985088000.0000 - 58ms/epoch - 5ms/step\n",
      "Epoch 5/200\n",
      "12/12 - 0s - loss: 932980476242349260800.0000 - mae: 1685176320.0000 - val_loss: 60406709744326424920064.0000 - val_mae: 20985159680.0000 - 59ms/epoch - 5ms/step\n",
      "Epoch 6/200\n",
      "12/12 - 0s - loss: 932978928129977352192.0000 - mae: 1685271680.0000 - val_loss: 60406682722728660697088.0000 - val_mae: 20985237504.0000 - 58ms/epoch - 5ms/step\n",
      "Epoch 7/200\n",
      "12/12 - 0s - loss: 932977098542628732928.0000 - mae: 1685389312.0000 - val_loss: 60406651197531269103616.0000 - val_mae: 20985331712.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 8/200\n",
      "12/12 - 0s - loss: 932975198586535936000.0000 - mae: 1685501952.0000 - val_loss: 60406633183132759621632.0000 - val_mae: 20985442304.0000 - 53ms/epoch - 4ms/step\n",
      "Epoch 9/200\n",
      "12/12 - 0s - loss: 932972806049233895424.0000 - mae: 1685643776.0000 - val_loss: 60406597154335740657664.0000 - val_mae: 20985550848.0000 - 48ms/epoch - 4ms/step\n",
      "Epoch 10/200\n",
      "12/12 - 0s - loss: 932970976461885276160.0000 - mae: 1685842944.0000 - val_loss: 60406552118339466952704.0000 - val_mae: 20985724928.0000 - 48ms/epoch - 4ms/step\n",
      "Epoch 11/200\n",
      "12/12 - 0s - loss: 932968513555839057920.0000 - mae: 1686054912.0000 - val_loss: 60406493571544311136256.0000 - val_mae: 20985899008.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 12/200\n",
      "12/12 - 0s - loss: 932964432168676753408.0000 - mae: 1686246400.0000 - val_loss: 60406444031948410060800.0000 - val_mae: 20986017792.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 13/200\n",
      "12/12 - 0s - loss: 932960772993979514880.0000 - mae: 1686422912.0000 - val_loss: 60406376477953999503360.0000 - val_mae: 20986165248.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 14/200\n",
      "12/12 - 0s - loss: 932955073125701124096.0000 - mae: 1686567808.0000 - val_loss: 60406317931158843686912.0000 - val_mae: 20986261504.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 15/200\n",
      "12/12 - 0s - loss: 932951765794724773888.0000 - mae: 1686925440.0000 - val_loss: 60406196333968904683520.0000 - val_mae: 20986714112.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 16/200\n",
      "12/12 - 0s - loss: 932944095601609408512.0000 - mae: 1687435136.0000 - val_loss: 60406106261976357273600.0000 - val_mae: 20986978304.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 17/200\n",
      "12/12 - 0s - loss: 932936777252214931456.0000 - mae: 1687734272.0000 - val_loss: 60406038707981946716160.0000 - val_mae: 20987133952.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 18/200\n",
      "12/12 - 0s - loss: 932933469921238581248.0000 - mae: 1688133632.0000 - val_loss: 60405939628790144565248.0000 - val_mae: 20987394048.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 19/200\n",
      "12/12 - 0s - loss: 932925236778169794560.0000 - mae: 1688397184.0000 - val_loss: 60405867571196106637312.0000 - val_mae: 20987537408.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 20/200\n",
      "12/12 - 0s - loss: 932915736997705809920.0000 - mae: 1688908416.0000 - val_loss: 60405705441609521299456.0000 - val_mae: 20988106752.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 21/200\n",
      "12/12 - 0s - loss: 932909333441985642496.0000 - mae: 1689788032.0000 - val_loss: 60405565830021072814080.0000 - val_mae: 20988481536.0000 - 51ms/epoch - 4ms/step\n",
      "Epoch 22/200\n",
      "12/12 - 0s - loss: 932899552186544947200.0000 - mae: 1690157824.0000 - val_loss: 60405466750829270663168.0000 - val_mae: 20988626944.0000 - 48ms/epoch - 4ms/step\n",
      "Epoch 23/200\n",
      "12/12 - 0s - loss: 932892867155848069120.0000 - mae: 1690528000.0000 - val_loss: 60405340650039704289280.0000 - val_mae: 20988899328.0000 - 50ms/epoch - 4ms/step\n",
      "Epoch 24/200\n",
      "12/12 - 0s - loss: 932884845119011815424.0000 - mae: 1691028736.0000 - val_loss: 60405219052849765285888.0000 - val_mae: 20989267968.0000 - 50ms/epoch - 4ms/step\n",
      "Epoch 25/200\n",
      "12/12 - 0s - loss: 932875697182268719104.0000 - mae: 1691684096.0000 - val_loss: 60405079441261316800512.0000 - val_mae: 20989704192.0000 - 48ms/epoch - 4ms/step\n",
      "Epoch 26/200\n",
      "12/12 - 0s - loss: 932867323301711577088.0000 - mae: 1692351104.0000 - val_loss: 60404930822473613574144.0000 - val_mae: 20990132224.0000 - 48ms/epoch - 4ms/step\n",
      "Epoch 27/200\n",
      "12/12 - 0s - loss: 932855290246457196544.0000 - mae: 1692950016.0000 - val_loss: 60404777700086282977280.0000 - val_mae: 20990498816.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 28/200\n",
      "12/12 - 0s - loss: 932844031247388770304.0000 - mae: 1693716608.0000 - val_loss: 60404611066900070268928.0000 - val_mae: 20990998528.0000 - 48ms/epoch - 4ms/step\n",
      "Epoch 29/200\n",
      "12/12 - 0s - loss: 932835798104319983616.0000 - mae: 1694742912.0000 - val_loss: 60404376879719447003136.0000 - val_mae: 20991787008.0000 - 48ms/epoch - 4ms/step\n",
      "Epoch 30/200\n",
      "12/12 - 0s - loss: 932817150387112902656.0000 - mae: 1695631872.0000 - val_loss: 60404201239333979553792.0000 - val_mae: 20992208896.0000 - 53ms/epoch - 4ms/step\n",
      "Epoch 31/200\n",
      "12/12 - 0s - loss: 932803287744509902848.0000 - mae: 1696515712.0000 - val_loss: 60403994073751120510976.0000 - val_mae: 20992942080.0000 - 56ms/epoch - 5ms/step\n",
      "Epoch 32/200\n",
      "12/12 - 0s - loss: 932790762108046278656.0000 - mae: 1697852928.0000 - val_loss: 60403714850574223540224.0000 - val_mae: 20993855488.0000 - 86ms/epoch - 7ms/step\n",
      "Epoch 33/200\n",
      "12/12 - 0s - loss: 932769862591025512448.0000 - mae: 1698768640.0000 - val_loss: 60403539210188756090880.0000 - val_mae: 20994150400.0000 - 68ms/epoch - 6ms/step\n",
      "Epoch 34/200\n",
      "12/12 - 0s - loss: 932760222073073172480.0000 - mae: 1700274560.0000 - val_loss: 60403174418618939080704.0000 - val_mae: 20995414016.0000 - 50ms/epoch - 4ms/step\n",
      "Epoch 35/200\n",
      "12/12 - 0s - loss: 932731722731681218560.0000 - mae: 1701437824.0000 - val_loss: 60402935727838688444416.0000 - val_mae: 20995975168.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 36/200\n",
      "12/12 - 0s - loss: 932699493846847848448.0000 - mae: 1703683200.0000 - val_loss: 60402408806682286096384.0000 - val_mae: 20997949440.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 37/200\n",
      "12/12 - 0s - loss: 932681057235873300480.0000 - mae: 1705831424.0000 - val_loss: 60401976461118058528768.0000 - val_mae: 20998828032.0000 - 48ms/epoch - 4ms/step\n",
      "Epoch 38/200\n",
      "12/12 - 0s - loss: 932655935594201874432.0000 - mae: 1707464960.0000 - val_loss: 60401652201944887853056.0000 - val_mae: 20999872512.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 39/200\n",
      "12/12 - 0s - loss: 932637147139506438144.0000 - mae: 1709459200.0000 - val_loss: 60401296417574325583872.0000 - val_mae: 21001023488.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 40/200\n",
      "12/12 - 0s - loss: 932611603285369946112.0000 - mae: 1711107968.0000 - val_loss: 60400981165600409649152.0000 - val_mae: 21001902080.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 41/200\n",
      "12/12 - 0s - loss: 932588522337279672320.0000 - mae: 1712252032.0000 - val_loss: 60400661410026866343936.0000 - val_mae: 21002833920.0000 - 49ms/epoch - 4ms/step\n",
      "Epoch 42/200\n",
      "12/12 - 0s - loss: 932564385858026733568.0000 - mae: 1713743872.0000 - val_loss: 60400391194049224114176.0000 - val_mae: 21003689984.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 43/200\n",
      "12/12 - 0s - loss: 932545386297098764288.0000 - mae: 1715690624.0000 - val_loss: 60399936330486859694080.0000 - val_mae: 21005277184.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 44/200\n",
      "12/12 - 0s - loss: 932512735199800328192.0000 - mae: 1717769216.0000 - val_loss: 60399558028118160572416.0000 - val_mae: 21006438400.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 45/200\n",
      "12/12 - 0s - loss: 932488739458035744768.0000 - mae: 1719642112.0000 - val_loss: 60399121178954305634304.0000 - val_mae: 21007980544.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 46/200\n",
      "12/12 - 0s - loss: 932458621635527704576.0000 - mae: 1721992320.0000 - val_loss: 60398639293794176991232.0000 - val_mae: 21009526784.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 47/200\n",
      "12/12 - 0s - loss: 932429066762973085696.0000 - mae: 1724736384.0000 - val_loss: 60398125883436656754688.0000 - val_mae: 21011570688.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 48/200\n",
      "12/12 - 0s - loss: 932394867553302740992.0000 - mae: 1727180160.0000 - val_loss: 60397653005475782852608.0000 - val_mae: 21013061632.0000 - 50ms/epoch - 4ms/step\n",
      "Epoch 49/200\n",
      "12/12 - 0s - loss: 932378330898420989952.0000 - mae: 1730613376.0000 - val_loss: 60396945940334285684736.0000 - val_mae: 21015443456.0000 - 49ms/epoch - 4ms/step\n",
      "Epoch 50/200\n",
      "12/12 - 0s - loss: 932320980371916193792.0000 - mae: 1733085824.0000 - val_loss: 60396513594770058117120.0000 - val_mae: 21017083904.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 51/200\n",
      "12/12 - 0s - loss: 932296351311454011392.0000 - mae: 1736355584.0000 - val_loss: 60395824544027070431232.0000 - val_mae: 21019461632.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 52/200\n",
      "12/12 - 0s - loss: 932245896921878626304.0000 - mae: 1739262080.0000 - val_loss: 60395329148068059676672.0000 - val_mae: 21021380608.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 53/200\n",
      "12/12 - 0s - loss: 932207616325045977088.0000 - mae: 1742073344.0000 - val_loss: 60394802226911657328640.0000 - val_mae: 21023156224.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 54/200\n",
      "12/12 - 0s - loss: 932189390820303962112.0000 - mae: 1747529984.0000 - val_loss: 60393469161421955661824.0000 - val_mae: 21027807232.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 55/200\n",
      "12/12 - 0s - loss: 932099811408965795840.0000 - mae: 1752708608.0000 - val_loss: 60392681031487165825024.0000 - val_mae: 21030803456.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 56/200\n",
      "12/12 - 0s - loss: 932056675368784887808.0000 - mae: 1757151616.0000 - val_loss: 60391888397952748617728.0000 - val_mae: 21033365504.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 57/200\n",
      "12/12 - 0s - loss: 932009809785162563584.0000 - mae: 1760782592.0000 - val_loss: 60391149807613859856384.0000 - val_mae: 21035939840.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 58/200\n",
      "12/12 - 0s - loss: 931952177783681056768.0000 - mae: 1763653888.0000 - val_loss: 60390699447651122806784.0000 - val_mae: 21038307328.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 59/200\n",
      "12/12 - 0s - loss: 931922059961173016576.0000 - mae: 1767754752.0000 - val_loss: 60389785216926766596096.0000 - val_mae: 21042081792.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 60/200\n",
      "12/12 - 0s - loss: 931867031603226083328.0000 - mae: 1771960704.0000 - val_loss: 60389001590591604129792.0000 - val_mae: 21045719040.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 61/200\n",
      "12/12 - 0s - loss: 931828962112625967104.0000 - mae: 1777025792.0000 - val_loss: 60388046827470601584640.0000 - val_mae: 21049944064.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 62/200\n",
      "12/12 - 0s - loss: 931762956230587318272.0000 - mae: 1780296320.0000 - val_loss: 60387528913513453977600.0000 - val_mae: 21052438528.0000 - 50ms/epoch - 4ms/step\n",
      "Epoch 63/200\n",
      "12/12 - 0s - loss: 931723127521382760448.0000 - mae: 1784186240.0000 - val_loss: 60386637200787234619392.0000 - val_mae: 21056573440.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 64/200\n",
      "12/12 - 0s - loss: 931664228882506055680.0000 - mae: 1788549120.0000 - val_loss: 60385934639245364822016.0000 - val_mae: 21060112384.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 65/200\n",
      "12/12 - 0s - loss: 931623203904650477568.0000 - mae: 1792810880.0000 - val_loss: 60384921329329206460416.0000 - val_mae: 21064992768.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 66/200\n",
      "12/12 - 0s - loss: 931595197144467767296.0000 - mae: 1800004480.0000 - val_loss: 60383732379027580649472.0000 - val_mae: 21070911488.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 67/200\n",
      "12/12 - 0s - loss: 931508362114152529920.0000 - mae: 1804096256.0000 - val_loss: 60383074853481984557056.0000 - val_mae: 21074237440.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 68/200\n",
      "12/12 - 0s - loss: 931464381649041489920.0000 - mae: 1809099392.0000 - val_loss: 60382070550765080936448.0000 - val_mae: 21078976512.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 69/200\n",
      "12/12 - 0s - loss: 931401823835467546624.0000 - mae: 1813336192.0000 - val_loss: 60381250895632899506176.0000 - val_mae: 21082464256.0000 - 49ms/epoch - 4ms/step\n",
      "Epoch 70/200\n",
      "12/12 - 0s - loss: 931352706452031537152.0000 - mae: 1818242432.0000 - val_loss: 60380264607314505367552.0000 - val_mae: 21087535104.0000 - 55ms/epoch - 5ms/step\n",
      "Epoch 71/200\n",
      "12/12 - 0s - loss: 931311048155478360064.0000 - mae: 1825159168.0000 - val_loss: 60379093671411389038592.0000 - val_mae: 21093330944.0000 - 65ms/epoch - 5ms/step\n",
      "Epoch 72/200\n",
      "12/12 - 0s - loss: 931231320368325066752.0000 - mae: 1829304320.0000 - val_loss: 60378368591871382388736.0000 - val_mae: 21096935424.0000 - 63ms/epoch - 5ms/step\n",
      "Epoch 73/200\n",
      "12/12 - 0s - loss: 931191984240329752576.0000 - mae: 1834232448.0000 - val_loss: 60377251699163794505728.0000 - val_mae: 21102608384.0000 - 63ms/epoch - 5ms/step\n",
      "Epoch 74/200\n",
      "12/12 - 0s - loss: 931119504433826758656.0000 - mae: 1839495040.0000 - val_loss: 60376450058430122557440.0000 - val_mae: 21106970624.0000 - 64ms/epoch - 5ms/step\n",
      "Epoch 75/200\n",
      "12/12 - 0s - loss: 931076368393645850624.0000 - mae: 1845092224.0000 - val_loss: 60375234086530732523520.0000 - val_mae: 21113491456.0000 - 56ms/epoch - 5ms/step\n",
      "Epoch 76/200\n",
      "12/12 - 0s - loss: 931002199737282592768.0000 - mae: 1850763904.0000 - val_loss: 60374306345007494201344.0000 - val_mae: 21118601216.0000 - 52ms/epoch - 4ms/step\n",
      "Epoch 77/200\n",
      "12/12 - 0s - loss: 930940767823615492096.0000 - mae: 1861630208.0000 - val_loss: 60371842876011322540032.0000 - val_mae: 21132478464.0000 - 51ms/epoch - 4ms/step\n",
      "Epoch 78/200\n",
      "12/12 - 0s - loss: 930835214707348996096.0000 - mae: 1872126336.0000 - val_loss: 60370428745728328204288.0000 - val_mae: 21139566592.0000 - 52ms/epoch - 4ms/step\n",
      "Epoch 79/200\n",
      "12/12 - 0s - loss: 930756260976381657088.0000 - mae: 1877924480.0000 - val_loss: 60369469479007698288640.0000 - val_mae: 21144037376.0000 - 56ms/epoch - 5ms/step\n",
      "Epoch 80/200\n",
      "12/12 - 0s - loss: 930709395392759332864.0000 - mae: 1884422144.0000 - val_loss: 60368411133095266222080.0000 - val_mae: 21150283776.0000 - 49ms/epoch - 4ms/step\n",
      "Epoch 81/200\n",
      "12/12 - 0s - loss: 930651130072580227072.0000 - mae: 1889369344.0000 - val_loss: 60367532931167928975360.0000 - val_mae: 21155246080.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 82/200\n",
      "12/12 - 0s - loss: 930604123751469547520.0000 - mae: 1895237632.0000 - val_loss: 60366524124851397984256.0000 - val_mae: 21160839168.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 83/200\n",
      "12/12 - 0s - loss: 930543958475197644800.0000 - mae: 1900257792.0000 - val_loss: 60365758512914744999936.0000 - val_mae: 21165109248.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 84/200\n",
      "12/12 - 0s - loss: 930504411240969797632.0000 - mae: 1904741760.0000 - val_loss: 60364722685000449785856.0000 - val_mae: 21170489344.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 85/200\n",
      "12/12 - 0s - loss: 930467186175299813376.0000 - mae: 1910655104.0000 - val_loss: 60363515720300314492928.0000 - val_mae: 21177178112.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 86/200\n",
      "12/12 - 0s - loss: 930407583848981331968.0000 - mae: 1917720448.0000 - val_loss: 60362425849190490832896.0000 - val_mae: 21183694848.0000 - 49ms/epoch - 4ms/step\n",
      "Epoch 87/200\n",
      "12/12 - 0s - loss: 930350092584988180480.0000 - mae: 1924463232.0000 - val_loss: 60361353992479176654848.0000 - val_mae: 21189715968.0000 - 50ms/epoch - 4ms/step\n",
      "Epoch 88/200\n",
      "12/12 - 0s - loss: 930289364358762856448.0000 - mae: 1928858880.0000 - val_loss: 60360624409339542634496.0000 - val_mae: 21193609216.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 89/200\n",
      "12/12 - 0s - loss: 930248198643418923008.0000 - mae: 1933498624.0000 - val_loss: 60359705675015559053312.0000 - val_mae: 21198966784.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 90/200\n",
      "12/12 - 0s - loss: 930201684903517487104.0000 - mae: 1939026688.0000 - val_loss: 60358615803905735393280.0000 - val_mae: 21204867072.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 91/200\n",
      "12/12 - 0s - loss: 930149823139058548736.0000 - mae: 1945844096.0000 - val_loss: 60357476393200010657792.0000 - val_mae: 21211336704.0000 - 49ms/epoch - 4ms/step\n",
      "Epoch 92/200\n",
      "12/12 - 0s - loss: 930090924500181843968.0000 - mae: 1951735296.0000 - val_loss: 60356589184073418670080.0000 - val_mae: 21216397312.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 93/200\n",
      "12/12 - 0s - loss: 930039836791908859904.0000 - mae: 1956365952.0000 - val_loss: 60355701974946826682368.0000 - val_mae: 21221031936.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 94/200\n",
      "12/12 - 0s - loss: 929995856326797819904.0000 - mae: 1962031872.0000 - val_loss: 60354603096637748281344.0000 - val_mae: 21227526144.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 95/200\n",
      "12/12 - 0s - loss: 929948075949501186048.0000 - mae: 1969415808.0000 - val_loss: 60353229498751400280064.0000 - val_mae: 21235089408.0000 - 50ms/epoch - 4ms/step\n",
      "Epoch 96/200\n",
      "12/12 - 0s - loss: 929907543552854851584.0000 - mae: 1977363328.0000 - val_loss: 60351788346870641721344.0000 - val_mae: 21243332608.0000 - 55ms/epoch - 5ms/step\n",
      "Epoch 97/200\n",
      "12/12 - 0s - loss: 929827323184492314624.0000 - mae: 1984198912.0000 - val_loss: 60350914648542931845120.0000 - val_mae: 21248555008.0000 - 70ms/epoch - 6ms/step\n",
      "Epoch 98/200\n",
      "12/12 - 0s - loss: 929794249874728812544.0000 - mae: 1990902912.0000 - val_loss: 60349595093852112289792.0000 - val_mae: 21256196096.0000 - 85ms/epoch - 7ms/step\n",
      "Epoch 99/200\n",
      "12/12 - 0s - loss: 929736547504503128064.0000 - mae: 1998096128.0000 - val_loss: 60348518733541170741248.0000 - val_mae: 21261930496.0000 - 62ms/epoch - 5ms/step\n",
      "Epoch 100/200\n",
      "12/12 - 0s - loss: 929693904045531463680.0000 - mae: 2003191936.0000 - val_loss: 60347307265241408077824.0000 - val_mae: 21268666368.0000 - 57ms/epoch - 5ms/step\n",
      "Epoch 101/200\n",
      "12/12 - 0s - loss: 929634512825445515264.0000 - mae: 2009831808.0000 - val_loss: 60346248919328976011264.0000 - val_mae: 21275006976.0000 - 57ms/epoch - 5ms/step\n",
      "Epoch 102/200\n",
      "12/12 - 0s - loss: 929598132184705662976.0000 - mae: 2018338944.0000 - val_loss: 60344947379036665937920.0000 - val_mae: 21282762752.0000 - 52ms/epoch - 4ms/step\n",
      "Epoch 103/200\n",
      "12/12 - 0s - loss: 929533041096341323776.0000 - mae: 2023846144.0000 - val_loss: 60344006126714545504256.0000 - val_mae: 21288112128.0000 - 49ms/epoch - 4ms/step\n",
      "Epoch 104/200\n",
      "12/12 - 0s - loss: 929499615942856933376.0000 - mae: 2031027584.0000 - val_loss: 60342731608019999653888.0000 - val_mae: 21295603712.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 105/200\n",
      "12/12 - 0s - loss: 929434384117004238848.0000 - mae: 2036660992.0000 - val_loss: 60341929967286327705600.0000 - val_mae: 21300172800.0000 - 48ms/epoch - 4ms/step\n",
      "Epoch 106/200\n",
      "12/12 - 0s - loss: 929402366338403401728.0000 - mae: 2040472832.0000 - val_loss: 60341240916543340019712.0000 - val_mae: 21304475648.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 107/200\n",
      "12/12 - 0s - loss: 929388855539521290240.0000 - mae: 2048497920.0000 - val_loss: 60339525045085311860736.0000 - val_mae: 21313841152.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 108/200\n",
      "12/12 - 0s - loss: 929302653827903651840.0000 - mae: 2055128704.0000 - val_loss: 60338678368355366207488.0000 - val_mae: 21319120896.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 109/200\n",
      "12/12 - 0s - loss: 929257336356653236224.0000 - mae: 2060031744.0000 - val_loss: 60337710094435481550848.0000 - val_mae: 21324953600.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 110/200\n",
      "12/12 - 0s - loss: 929186545400010506240.0000 - mae: 2074010112.0000 - val_loss: 60334332394714953678848.0000 - val_mae: 21343827968.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 111/200\n",
      "12/12 - 0s - loss: 929087818051929243648.0000 - mae: 2087770752.0000 - val_loss: 60332747127646119264256.0000 - val_mae: 21353351168.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 112/200\n",
      "12/12 - 0s - loss: 929034126700121686016.0000 - mae: 2096509696.0000 - val_loss: 60331693285333314568192.0000 - val_mae: 21359996928.0000 - 53ms/epoch - 4ms/step\n",
      "Epoch 113/200\n",
      "12/12 - 0s - loss: 929024415813425168384.0000 - mae: 2104402304.0000 - val_loss: 60330328694646221307904.0000 - val_mae: 21367209984.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 114/200\n",
      "12/12 - 0s - loss: 928954258175480037376.0000 - mae: 2110456192.0000 - val_loss: 60329576593508450435072.0000 - val_mae: 21371654144.0000 - 48ms/epoch - 4ms/step\n",
      "Epoch 115/200\n",
      "12/12 - 0s - loss: 928939269632970194944.0000 - mae: 2114870912.0000 - val_loss: 60328509240396763627520.0000 - val_mae: 21377900544.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 116/200\n",
      "12/12 - 0s - loss: 928885156068697571328.0000 - mae: 2119141888.0000 - val_loss: 60327806678854893830144.0000 - val_mae: 21381935104.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 117/200\n",
      "12/12 - 0s - loss: 928813872530845597696.0000 - mae: 2126886912.0000 - val_loss: 60325861123815869775872.0000 - val_mae: 21393418240.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 118/200\n",
      "12/12 - 0s - loss: 928787062039313907712.0000 - mae: 2138212864.0000 - val_loss: 60324284863946290102272.0000 - val_mae: 21402370048.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 119/200\n",
      "12/12 - 0s - loss: 928747796280062771200.0000 - mae: 2146779904.0000 - val_loss: 60323131942441683255296.0000 - val_mae: 21409732608.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 120/200\n",
      "12/12 - 0s - loss: 928715004445275979776.0000 - mae: 2154348544.0000 - val_loss: 60321974517337449037824.0000 - val_mae: 21415815168.0000 - 50ms/epoch - 4ms/step\n",
      "Epoch 121/200\n",
      "12/12 - 0s - loss: 928662157518398554112.0000 - mae: 2157235712.0000 - val_loss: 60321690790560924696576.0000 - val_mae: 21417596928.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 122/200\n",
      "12/12 - 0s - loss: 928649842988167462912.0000 - mae: 2162047488.0000 - val_loss: 60320488329460416774144.0000 - val_mae: 21424572416.0000 - 51ms/epoch - 4ms/step\n",
      "Epoch 123/200\n",
      "12/12 - 0s - loss: 928609943910218727424.0000 - mae: 2168356608.0000 - val_loss: 60319601120333824786432.0000 - val_mae: 21429534720.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 124/200\n",
      "12/12 - 0s - loss: 928582077887524372480.0000 - mae: 2171104000.0000 - val_loss: 60319240832363635146752.0000 - val_mae: 21431736320.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 125/200\n",
      "12/12 - 0s - loss: 928580107562687397888.0000 - mae: 2176264704.0000 - val_loss: 60317934788471697702912.0000 - val_mae: 21439150080.0000 - 50ms/epoch - 4ms/step\n",
      "Epoch 126/200\n",
      "12/12 - 0s - loss: 928531482760460632064.0000 - mae: 2184007680.0000 - val_loss: 60317151162136535236608.0000 - val_mae: 21444405248.0000 - 60ms/epoch - 5ms/step\n",
      "Epoch 127/200\n",
      "12/12 - 0s - loss: 928513609099439505408.0000 - mae: 2189281280.0000 - val_loss: 60316182888216650579968.0000 - val_mae: 21449603072.0000 - 59ms/epoch - 5ms/step\n",
      "Epoch 128/200\n",
      "12/12 - 0s - loss: 928489050407721500672.0000 - mae: 2192332032.0000 - val_loss: 60315552384268818710528.0000 - val_mae: 21453273088.0000 - 59ms/epoch - 5ms/step\n",
      "Epoch 129/200\n",
      "12/12 - 0s - loss: 928466532409584648192.0000 - mae: 2195797248.0000 - val_loss: 60315011952313534251008.0000 - val_mae: 21456209920.0000 - 58ms/epoch - 5ms/step\n",
      "Epoch 130/200\n",
      "12/12 - 0s - loss: 928464069503538429952.0000 - mae: 2203211264.0000 - val_loss: 60313741937218615771136.0000 - val_mae: 21463885824.0000 - 63ms/epoch - 5ms/step\n",
      "Epoch 131/200\n",
      "12/12 - 0s - loss: 928411433682893537280.0000 - mae: 2208829184.0000 - val_loss: 60312881749689788006400.0000 - val_mae: 21468899328.0000 - 59ms/epoch - 5ms/step\n",
      "Epoch 132/200\n",
      "12/12 - 0s - loss: 928379556641781055488.0000 - mae: 2212819200.0000 - val_loss: 60312264756540838248448.0000 - val_mae: 21472571392.0000 - 51ms/epoch - 4ms/step\n",
      "Epoch 133/200\n",
      "12/12 - 0s - loss: 928366327317875654656.0000 - mae: 2217581056.0000 - val_loss: 60311436094209402077184.0000 - val_mae: 21477740544.0000 - 49ms/epoch - 4ms/step\n",
      "Epoch 134/200\n",
      "12/12 - 0s - loss: 928334309539274817536.0000 - mae: 2220696064.0000 - val_loss: 60310958712648900804608.0000 - val_mae: 21480011776.0000 - 53ms/epoch - 4ms/step\n",
      "Epoch 135/200\n",
      "12/12 - 0s - loss: 928337898345227878400.0000 - mae: 2225614592.0000 - val_loss: 60309787776745784475648.0000 - val_mae: 21486594048.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 136/200\n",
      "12/12 - 0s - loss: 928314958134625959936.0000 - mae: 2233867776.0000 - val_loss: 60308711416434842927104.0000 - val_mae: 21493252096.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 137/200\n",
      "12/12 - 0s - loss: 928279070075095351296.0000 - mae: 2235359488.0000 - val_loss: 60308720423634097668096.0000 - val_mae: 21493356544.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 138/200\n",
      "12/12 - 0s - loss: 928252963271005437952.0000 - mae: 2238606080.0000 - val_loss: 60307896264902288867328.0000 - val_mae: 21498368000.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 139/200\n",
      "12/12 - 0s - loss: 928249515202540732416.0000 - mae: 2241054720.0000 - val_loss: 60307639559723528749056.0000 - val_mae: 21499392000.0000 - 50ms/epoch - 4ms/step\n",
      "Epoch 140/200\n",
      "12/12 - 0s - loss: 928222141761055621120.0000 - mae: 2243117312.0000 - val_loss: 60307085616969362178048.0000 - val_mae: 21502779392.0000 - 50ms/epoch - 4ms/step\n",
      "Epoch 141/200\n",
      "12/12 - 0s - loss: 928217497423939895296.0000 - mae: 2250819840.0000 - val_loss: 60305851630671462662144.0000 - val_mae: 21509892096.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 142/200\n",
      "12/12 - 0s - loss: 928191249882361626624.0000 - mae: 2252622848.0000 - val_loss: 60305509357099782504448.0000 - val_mae: 21511323648.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 143/200\n",
      "12/12 - 0s - loss: 928166268978178555904.0000 - mae: 2263107584.0000 - val_loss: 60302744146928577019904.0000 - val_mae: 21528588288.0000 - 49ms/epoch - 4ms/step\n",
      "Epoch 144/200\n",
      "12/12 - 0s - loss: 928094070646652272640.0000 - mae: 2274375936.0000 - val_loss: 60301843427003102920704.0000 - val_mae: 21533517824.0000 - 48ms/epoch - 4ms/step\n",
      "Epoch 145/200\n",
      "12/12 - 0s - loss: 928089074465815658496.0000 - mae: 2281310208.0000 - val_loss: 60300658980301104480256.0000 - val_mae: 21540777984.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 146/200\n",
      "12/12 - 0s - loss: 928054171568703537152.0000 - mae: 2286040320.0000 - val_loss: 60299960922358862053376.0000 - val_mae: 21544312832.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 147/200\n",
      "12/12 - 0s - loss: 928040871876053958656.0000 - mae: 2290822912.0000 - val_loss: 60299141267226680623104.0000 - val_mae: 21549119488.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 148/200\n",
      "12/12 - 0s - loss: 928030386933171486720.0000 - mae: 2295494656.0000 - val_loss: 60298353137291890786304.0000 - val_mae: 21554286592.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 149/200\n",
      "12/12 - 0s - loss: 928004209760337395712.0000 - mae: 2299556608.0000 - val_loss: 60298037885317974851584.0000 - val_mae: 21556045824.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 150/200\n",
      "12/12 - 0s - loss: 927998158048338116608.0000 - mae: 2302434560.0000 - val_loss: 60297204719386911309824.0000 - val_mae: 21560152064.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 151/200\n",
      "12/12 - 0s - loss: 927979580699875213312.0000 - mae: 2305472256.0000 - val_loss: 60296749855824546889728.0000 - val_mae: 21563662336.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 152/200\n",
      "12/12 - 0s - loss: 927967336538388299776.0000 - mae: 2309194752.0000 - val_loss: 60296047294282677092352.0000 - val_mae: 21567309824.0000 - 49ms/epoch - 4ms/step\n",
      "Epoch 153/200\n",
      "12/12 - 0s - loss: 927948196239971975168.0000 - mae: 2313395968.0000 - val_loss: 60295313207543415701504.0000 - val_mae: 21571796992.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 154/200\n",
      "12/12 - 0s - loss: 927932292903787823104.0000 - mae: 2317348608.0000 - val_loss: 60294835825982914428928.0000 - val_mae: 21574975488.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 155/200\n",
      "12/12 - 0s - loss: 927922863492068016128.0000 - mae: 2321994752.0000 - val_loss: 60293962127655204552704.0000 - val_mae: 21580118016.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 156/200\n",
      "12/12 - 0s - loss: 927907171262116397056.0000 - mae: 2324986624.0000 - val_loss: 60293818012467128696832.0000 - val_mae: 21581236224.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 157/200\n",
      "12/12 - 0s - loss: 927898023325373300736.0000 - mae: 2329316864.0000 - val_loss: 60293133465323768381440.0000 - val_mae: 21585274880.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 158/200\n",
      "12/12 - 0s - loss: 927884371789002833920.0000 - mae: 2332648704.0000 - val_loss: 60292268774195313246208.0000 - val_mae: 21589624832.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 159/200\n",
      "12/12 - 0s - loss: 927874097952352894976.0000 - mae: 2337658880.0000 - val_loss: 60291368054269839147008.0000 - val_mae: 21594798080.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 160/200\n",
      "12/12 - 0s - loss: 927851720691704397824.0000 - mae: 2341024000.0000 - val_loss: 60291070816694432694272.0000 - val_mae: 21596579840.0000 - 76ms/epoch - 6ms/step\n",
      "Epoch 161/200\n",
      "12/12 - 0s - loss: 927848202254495514624.0000 - mae: 2344143104.0000 - val_loss: 60290255665161878634496.0000 - val_mae: 21601431552.0000 - 64ms/epoch - 5ms/step\n",
      "Epoch 162/200\n",
      "12/12 - 0s - loss: 927848131885751336960.0000 - mae: 2350177280.0000 - val_loss: 60289336930837895053312.0000 - val_mae: 21606928384.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 163/200\n",
      "12/12 - 0s - loss: 927822376925382311936.0000 - mae: 2354580736.0000 - val_loss: 60288778484484101111808.0000 - val_mae: 21610186752.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 164/200\n",
      "12/12 - 0s - loss: 927824276881475108864.0000 - mae: 2355201280.0000 - val_loss: 60288832527679629557760.0000 - val_mae: 21610145792.0000 - 49ms/epoch - 4ms/step\n",
      "Epoch 165/200\n",
      "12/12 - 0s - loss: 927830610068451098624.0000 - mae: 2359230720.0000 - val_loss: 60287877764558627012608.0000 - val_mae: 21615591424.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 166/200\n",
      "12/12 - 0s - loss: 927812666038685794304.0000 - mae: 2360361984.0000 - val_loss: 60288220038130307170304.0000 - val_mae: 21613957120.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 167/200\n",
      "12/12 - 0s - loss: 927796270121292398592.0000 - mae: 2359980032.0000 - val_loss: 60287648080977631117312.0000 - val_mae: 21616687104.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 168/200\n",
      "12/12 - 0s - loss: 927785151859712327680.0000 - mae: 2363826432.0000 - val_loss: 60287062613026072952832.0000 - val_mae: 21620480000.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 169/200\n",
      "12/12 - 0s - loss: 927777974247806205952.0000 - mae: 2368247296.0000 - val_loss: 60286486152273769529344.0000 - val_mae: 21623703552.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 170/200\n",
      "12/12 - 0s - loss: 927791344309199962112.0000 - mae: 2373398528.0000 - val_loss: 60285639475543823876096.0000 - val_mae: 21628557312.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 171/200\n",
      "12/12 - 0s - loss: 927758763580645703680.0000 - mae: 2377472512.0000 - val_loss: 60285157590383695233024.0000 - val_mae: 21632557056.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 172/200\n",
      "12/12 - 0s - loss: 927745112044275236864.0000 - mae: 2377885696.0000 - val_loss: 60285301705571771088896.0000 - val_mae: 21630771200.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 173/200\n",
      "12/12 - 0s - loss: 927740045494694445056.0000 - mae: 2378759424.0000 - val_loss: 60284617158428410773504.0000 - val_mae: 21634889728.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 174/200\n",
      "12/12 - 0s - loss: 927742015819531419648.0000 - mae: 2380261376.0000 - val_loss: 60284725244819467665408.0000 - val_mae: 21635088384.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 175/200\n",
      "12/12 - 0s - loss: 927722594046138384384.0000 - mae: 2382502656.0000 - val_loss: 60284202827262692687872.0000 - val_mae: 21637545984.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 176/200\n",
      "12/12 - 0s - loss: 927716331227906572288.0000 - mae: 2384992256.0000 - val_loss: 60283558812515978706944.0000 - val_mae: 21641377792.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 177/200\n",
      "12/12 - 0s - loss: 927713235003162755072.0000 - mae: 2389478144.0000 - val_loss: 60282811214977835204608.0000 - val_mae: 21645486080.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 178/200\n",
      "12/12 - 0s - loss: 927704298172652191744.0000 - mae: 2392804352.0000 - val_loss: 60282356351415470784512.0000 - val_mae: 21648424960.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 179/200\n",
      "12/12 - 0s - loss: 927695783554606694400.0000 - mae: 2397321728.0000 - val_loss: 60281766379864285249536.0000 - val_mae: 21651908608.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 180/200\n",
      "12/12 - 0s - loss: 927686002299165999104.0000 - mae: 2398052608.0000 - val_loss: 60281685315070992580608.0000 - val_mae: 21651953664.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 181/200\n",
      "12/12 - 0s - loss: 927689661473863237632.0000 - mae: 2399951872.0000 - val_loss: 60280996264328004894720.0000 - val_mae: 21656201216.0000 - 49ms/epoch - 4ms/step\n",
      "Epoch 182/200\n",
      "12/12 - 0s - loss: 927674602562609217536.0000 - mae: 2403162368.0000 - val_loss: 60280739559149244776448.0000 - val_mae: 21657913344.0000 - 51ms/epoch - 4ms/step\n",
      "Epoch 183/200\n",
      "12/12 - 0s - loss: 927666650894517141504.0000 - mae: 2405918720.0000 - val_loss: 60280239659590606651392.0000 - val_mae: 21660751872.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 184/200\n",
      "12/12 - 0s - loss: 927662428769866481664.0000 - mae: 2409116416.0000 - val_loss: 60279699227635322191872.0000 - val_mae: 21664350208.0000 - 48ms/epoch - 4ms/step\n",
      "Epoch 185/200\n",
      "12/12 - 0s - loss: 927685509717956755456.0000 - mae: 2419456000.0000 - val_loss: 60277262780236914753536.0000 - val_mae: 21678454784.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 186/200\n",
      "12/12 - 0s - loss: 927616196504941756416.0000 - mae: 2426312448.0000 - val_loss: 60276704333883120812032.0000 - val_mae: 21681561600.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 187/200\n",
      "12/12 - 0s - loss: 927587978638526513152.0000 - mae: 2435179776.0000 - val_loss: 60274875872434408390656.0000 - val_mae: 21692856320.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 188/200\n",
      "12/12 - 0s - loss: 927589597119642599424.0000 - mae: 2444048896.0000 - val_loss: 60273691425732409950208.0000 - val_mae: 21699958784.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 189/200\n",
      "12/12 - 0s - loss: 927575312264574533632.0000 - mae: 2447931648.0000 - val_loss: 60273344648561102422016.0000 - val_mae: 21701605376.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 190/200\n",
      "12/12 - 0s - loss: 927587486057317269504.0000 - mae: 2452303872.0000 - val_loss: 60272592547423331549184.0000 - val_mae: 21705734144.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 191/200\n",
      "12/12 - 0s - loss: 927564475477971173376.0000 - mae: 2454266880.0000 - val_loss: 60272538504227803103232.0000 - val_mae: 21706561536.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 192/200\n",
      "12/12 - 0s - loss: 927566656909040680960.0000 - mae: 2456286464.0000 - val_loss: 60272110662263202906112.0000 - val_mae: 21709023232.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 193/200\n",
      "12/12 - 0s - loss: 927561660728204066816.0000 - mae: 2456893952.0000 - val_loss: 60271993568672891273216.0000 - val_mae: 21709807616.0000 - 50ms/epoch - 4ms/step\n",
      "Epoch 194/200\n",
      "12/12 - 0s - loss: 927571512352388939776.0000 - mae: 2460344576.0000 - val_loss: 60271345050326549921792.0000 - val_mae: 21713362944.0000 - 68ms/epoch - 6ms/step\n",
      "Epoch 195/200\n",
      "12/12 - 0s - loss: 927557086759832518656.0000 - mae: 2460280576.0000 - val_loss: 60271372071924314144768.0000 - val_mae: 21713055744.0000 - 64ms/epoch - 5ms/step\n",
      "Epoch 196/200\n",
      "12/12 - 0s - loss: 927563771790529396736.0000 - mae: 2459807744.0000 - val_loss: 60271597251905682669568.0000 - val_mae: 21712650240.0000 - 62ms/epoch - 5ms/step\n",
      "Epoch 197/200\n",
      "12/12 - 0s - loss: nan - mae: nan - val_loss: nan - val_mae: nan - 63ms/epoch - 5ms/step\n",
      "Epoch 198/200\n",
      "12/12 - 0s - loss: nan - mae: nan - val_loss: nan - val_mae: nan - 62ms/epoch - 5ms/step\n",
      "Epoch 199/200\n",
      "12/12 - 0s - loss: nan - mae: nan - val_loss: nan - val_mae: nan - 64ms/epoch - 5ms/step\n",
      "Epoch 200/200\n",
      "12/12 - 0s - loss: nan - mae: nan - val_loss: nan - val_mae: nan - 62ms/epoch - 5ms/step\n"
     ]
    }
   ],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "xTrain = xTrain.astype(np.float32)\n",
    "xVal = xVal.astype(np.float32)\n",
    "yTrain = yTrain.astype(np.float32)\n",
    "yVal = yVal.astype(np.float32)\n",
    "\n",
    "history = model.fit(\n",
    "    xTrain, yTrain,\n",
    "    validation_data=(xVal, yVal),\n",
    "    epochs=200,\n",
    "    batch_size=16,\n",
    "    callbacks=[es],\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a32cf2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: flightPathModel\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('flightPathModel')\n",
    "import json\n",
    "with open('flightPathModel/columns.json', 'w') as f:\n",
    "    json.dump(list(x.columns), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
