{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3437961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b667345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('LaunchData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "96f07167",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['RocketName'])\n",
    "x = df.drop(columns=['AltitudeHeight', 'TimeAtAltitude', 'TotalFlightTime'])\n",
    "y = df[['AltitudeHeight', 'TimeAtAltitude', 'TotalFlightTime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f1a327da",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.get_dummies(x, columns=['NoseType', 'FinShape'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "eccf5348",
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xVal, yTrain, yVal = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fc6e5457",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(xTrain.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(3)  \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "24cc39a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "12/12 - 1s - loss: 932986950166813605888.0000 - mae: 1685649408.0000 - val_loss: 60406781801920462848000.0000 - val_mae: 20985933824.0000 - 535ms/epoch - 45ms/step\n",
      "Epoch 2/200\n",
      "12/12 - 0s - loss: 932983642835837255680.0000 - mae: 1685730816.0000 - val_loss: 60406727758724934402048.0000 - val_mae: 20986023936.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 3/200\n",
      "12/12 - 0s - loss: 932979842923651661824.0000 - mae: 1685822208.0000 - val_loss: 60406682722728660697088.0000 - val_mae: 20986095616.0000 - 42ms/epoch - 3ms/step\n",
      "Epoch 4/200\n",
      "12/12 - 0s - loss: 932976324486442778624.0000 - mae: 1685941120.0000 - val_loss: 60406633183132759621632.0000 - val_mae: 20986146816.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 5/200\n",
      "12/12 - 0s - loss: 932972383836768829440.0000 - mae: 1685983872.0000 - val_loss: 60406574636337603805184.0000 - val_mae: 20986169344.0000 - 42ms/epoch - 3ms/step\n",
      "Epoch 6/200\n",
      "12/12 - 0s - loss: 932969920930722611200.0000 - mae: 1686038784.0000 - val_loss: 60406493571544311136256.0000 - val_mae: 20986185728.0000 - 41ms/epoch - 3ms/step\n",
      "Epoch 7/200\n",
      "12/12 - 0s - loss: 932964432168676753408.0000 - mae: 1686053632.0000 - val_loss: 60406421513950273208320.0000 - val_mae: 20986155008.0000 - 42ms/epoch - 3ms/step\n",
      "Epoch 8/200\n",
      "12/12 - 0s - loss: 932958591562910007296.0000 - mae: 1686051584.0000 - val_loss: 60406371974354372132864.0000 - val_mae: 20986122240.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 9/200\n",
      "12/12 - 0s - loss: 932954862019468591104.0000 - mae: 1686130816.0000 - val_loss: 60406268391562942611456.0000 - val_mae: 20986169344.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 10/200\n",
      "12/12 - 0s - loss: 932947895513795002368.0000 - mae: 1686222208.0000 - val_loss: 60406182823170022572032.0000 - val_mae: 20986189824.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 11/200\n",
      "12/12 - 0s - loss: 932942899332958388224.0000 - mae: 1686462464.0000 - val_loss: 60406083743978220421120.0000 - val_mae: 20986304512.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 12/200\n",
      "12/12 - 0s - loss: 932936284671005687808.0000 - mae: 1686767744.0000 - val_loss: 60405966650387908788224.0000 - val_mae: 20986478592.0000 - 37ms/epoch - 3ms/step\n",
      "Epoch 13/200\n",
      "12/12 - 0s - loss: 932927981159192723456.0000 - mae: 1687073536.0000 - val_loss: 60405863067596479266816.0000 - val_mae: 20986683392.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 14/200\n",
      "12/12 - 0s - loss: 932920240597333180416.0000 - mae: 1687501184.0000 - val_loss: 60405732463207285522432.0000 - val_mae: 20986961920.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 15/200\n",
      "12/12 - 0s - loss: 932910388973148307456.0000 - mae: 1687971968.0000 - val_loss: 60405606362417719148544.0000 - val_mae: 20987279360.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 16/200\n",
      "12/12 - 0s - loss: 932896948543010373632.0000 - mae: 1688717312.0000 - val_loss: 60405403700434487476224.0000 - val_mae: 20987813888.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 17/200\n",
      "12/12 - 0s - loss: 932887237656313856000.0000 - mae: 1689643904.0000 - val_loss: 60405205542050883174400.0000 - val_mae: 20988424192.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 18/200\n",
      "12/12 - 0s - loss: 932873937963664277504.0000 - mae: 1690570240.0000 - val_loss: 60405043412464297836544.0000 - val_mae: 20988987392.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 19/200\n",
      "12/12 - 0s - loss: 932863312283293450240.0000 - mae: 1691383808.0000 - val_loss: 60404840750481066164224.0000 - val_mae: 20989554688.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 20/200\n",
      "12/12 - 0s - loss: 932848534847016140800.0000 - mae: 1692282752.0000 - val_loss: 60404678620894480826368.0000 - val_mae: 20990128128.0000 - 42ms/epoch - 3ms/step\n",
      "Epoch 21/200\n",
      "12/12 - 0s - loss: 932835798104319983616.0000 - mae: 1693257472.0000 - val_loss: 60404466951711994413056.0000 - val_mae: 20990869504.0000 - 49ms/epoch - 4ms/step\n",
      "Epoch 22/200\n",
      "12/12 - 0s - loss: 932821653986740273152.0000 - mae: 1694479744.0000 - val_loss: 60404237268130998517760.0000 - val_mae: 20991700992.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 23/200\n",
      "12/12 - 0s - loss: 932804554381905100800.0000 - mae: 1695599232.0000 - val_loss: 60404021095348884733952.0000 - val_mae: 20992358400.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 24/200\n",
      "12/12 - 0s - loss: 932794843495208583168.0000 - mae: 1697137920.0000 - val_loss: 60403674318177577205760.0000 - val_mae: 20993478656.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 25/200\n",
      "12/12 - 0s - loss: 932769158903583735808.0000 - mae: 1698661504.0000 - val_loss: 60403426620198071828480.0000 - val_mae: 20994355200.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 26/200\n",
      "12/12 - 0s - loss: 932755859210934157312.0000 - mae: 1700347520.0000 - val_loss: 60403075339427136929792.0000 - val_mae: 20995354624.0000 - 49ms/epoch - 4ms/step\n",
      "Epoch 27/200\n",
      "12/12 - 0s - loss: 932728696875681579008.0000 - mae: 1701606784.0000 - val_loss: 60402778101851730477056.0000 - val_mae: 20996329472.0000 - 41ms/epoch - 3ms/step\n",
      "Epoch 28/200\n",
      "12/12 - 0s - loss: 932703786340242685952.0000 - mae: 1702977920.0000 - val_loss: 60402516893073342988288.0000 - val_mae: 20997052416.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 29/200\n",
      "12/12 - 0s - loss: 932691190335034884096.0000 - mae: 1705904640.0000 - val_loss: 60401859367527746895872.0000 - val_mae: 20999469056.0000 - 41ms/epoch - 3ms/step\n",
      "Epoch 30/200\n",
      "12/12 - 0s - loss: 932649672775970062336.0000 - mae: 1708510592.0000 - val_loss: 60401463050760538292224.0000 - val_mae: 21000443904.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 31/200\n",
      "12/12 - 0s - loss: 932623214128159260672.0000 - mae: 1710190464.0000 - val_loss: 60401129784388112875520.0000 - val_mae: 21001474048.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 32/200\n",
      "12/12 - 0s - loss: 932597670274022768640.0000 - mae: 1711491072.0000 - val_loss: 60400895597207489609728.0000 - val_mae: 21002362880.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 33/200\n",
      "12/12 - 0s - loss: 932583174312722169856.0000 - mae: 1714320000.0000 - val_loss: 60400256086060402999296.0000 - val_mae: 21004425216.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 34/200\n",
      "12/12 - 0s - loss: 932542290072354947072.0000 - mae: 1716718976.0000 - val_loss: 60399859769293194395648.0000 - val_mae: 21005592576.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 35/200\n",
      "12/12 - 0s - loss: 932521742399055069184.0000 - mae: 1718718976.0000 - val_loss: 60399427423728966828032.0000 - val_mae: 21006987264.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 36/200\n",
      "12/12 - 0s - loss: 932487191345663836160.0000 - mae: 1720335104.0000 - val_loss: 60399121178954305634304.0000 - val_mae: 21008056320.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 37/200\n",
      "12/12 - 0s - loss: 932464321503806095360.0000 - mae: 1721744512.0000 - val_loss: 60398769898183370735616.0000 - val_mae: 21009176576.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 38/200\n",
      "12/12 - 0s - loss: 932437651749762760704.0000 - mae: 1723783552.0000 - val_loss: 60398405106613553725440.0000 - val_mae: 21010571264.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 39/200\n",
      "12/12 - 0s - loss: 932416752232741994496.0000 - mae: 1726066176.0000 - val_loss: 60397873681857524006912.0000 - val_mae: 21012340736.0000 - 42ms/epoch - 3ms/step\n",
      "Epoch 40/200\n",
      "12/12 - 0s - loss: 932383960397955203072.0000 - mae: 1728551552.0000 - val_loss: 60397427825494414327808.0000 - val_mae: 21014022144.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 41/200\n",
      "12/12 - 0s - loss: 932351238931912589312.0000 - mae: 1731197184.0000 - val_loss: 60396972961932049907712.0000 - val_mae: 21015658496.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 42/200\n",
      "12/12 - 0s - loss: 932322458115543924736.0000 - mae: 1733606912.0000 - val_loss: 60396437033576392818688.0000 - val_mae: 21017415680.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 43/200\n",
      "12/12 - 0s - loss: 932287836693408514048.0000 - mae: 1736030976.0000 - val_loss: 60396013695211419992064.0000 - val_mae: 21018998784.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 44/200\n",
      "12/12 - 0s - loss: 932259618826993270784.0000 - mae: 1739059712.0000 - val_loss: 60395405709261724975104.0000 - val_mae: 21021040640.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 45/200\n",
      "12/12 - 0s - loss: 932226826992206479360.0000 - mae: 1742288256.0000 - val_loss: 60394770701714265735168.0000 - val_mae: 21023367168.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 46/200\n",
      "12/12 - 0s - loss: 932180594727281754112.0000 - mae: 1745055872.0000 - val_loss: 60394306830952646574080.0000 - val_mae: 21024806912.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 47/200\n",
      "12/12 - 0s - loss: 932167435772120530944.0000 - mae: 1748493696.0000 - val_loss: 60393554729814875701248.0000 - val_mae: 21027614720.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 48/200\n",
      "12/12 - 0s - loss: 932109100083197247488.0000 - mae: 1751870080.0000 - val_loss: 60392987276261827018752.0000 - val_mae: 21029488640.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 49/200\n",
      "12/12 - 0s - loss: 932066808467946471424.0000 - mae: 1753986048.0000 - val_loss: 60392559434297226821632.0000 - val_mae: 21030959104.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 50/200\n",
      "12/12 - 0s - loss: 932039083182740471808.0000 - mae: 1757439744.0000 - val_loss: 60391802829559828578304.0000 - val_mae: 21033682944.0000 - 42ms/epoch - 3ms/step\n",
      "Epoch 51/200\n",
      "12/12 - 0s - loss: 931990388011769528320.0000 - mae: 1760938624.0000 - val_loss: 60391136296814977744896.0000 - val_mae: 21036292096.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 52/200\n",
      "12/12 - 0s - loss: 931946970496611909632.0000 - mae: 1764357248.0000 - val_loss: 60390474267669754281984.0000 - val_mae: 21038843904.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 53/200\n",
      "12/12 - 0s - loss: 931914952718011072512.0000 - mae: 1768982528.0000 - val_loss: 60389582554943534923776.0000 - val_mae: 21042925568.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 54/200\n",
      "12/12 - 0s - loss: 931860487310017560576.0000 - mae: 1773162624.0000 - val_loss: 60388825950206136680448.0000 - val_mae: 21046507520.0000 - 41ms/epoch - 3ms/step\n",
      "Epoch 55/200\n",
      "12/12 - 0s - loss: 931813903201371947008.0000 - mae: 1777778688.0000 - val_loss: 60387983777075818397696.0000 - val_mae: 21050365952.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 56/200\n",
      "12/12 - 0s - loss: 931763871024261627904.0000 - mae: 1782477056.0000 - val_loss: 60387164121943636967424.0000 - val_mae: 21054285824.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 57/200\n",
      "12/12 - 0s - loss: 931721579409010851840.0000 - mae: 1786912896.0000 - val_loss: 60386254394818908127232.0000 - val_mae: 21058375680.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 58/200\n",
      "12/12 - 0s - loss: 931653955045856116736.0000 - mae: 1790491008.0000 - val_loss: 60385628394470703628288.0000 - val_mae: 21061416960.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 59/200\n",
      "12/12 - 0s - loss: 931621303948557680640.0000 - mae: 1794760192.0000 - val_loss: 60384637602552682119168.0000 - val_mae: 21065879552.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 60/200\n",
      "12/12 - 0s - loss: 931551568523077615616.0000 - mae: 1798830336.0000 - val_loss: 60383980077007086026752.0000 - val_mae: 21069283328.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 61/200\n",
      "12/12 - 0s - loss: 931513006451268255744.0000 - mae: 1803516288.0000 - val_loss: 60382962263491300294656.0000 - val_mae: 21074307072.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 62/200\n",
      "12/12 - 0s - loss: 931463889067832246272.0000 - mae: 1809708928.0000 - val_loss: 60381858881582594523136.0000 - val_mae: 21079592960.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 63/200\n",
      "12/12 - 0s - loss: 931385357549329973248.0000 - mae: 1814177152.0000 - val_loss: 60381106780444823650304.0000 - val_mae: 21083783168.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 64/200\n",
      "12/12 - 0s - loss: 931290641219666837504.0000 - mae: 1822304384.0000 - val_loss: 60379134203808035373056.0000 - val_mae: 21093509120.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 65/200\n",
      "12/12 - 0s - loss: 931210209745071767552.0000 - mae: 1830657280.0000 - val_loss: 60378066850696348565504.0000 - val_mae: 21099112448.0000 - 40ms/epoch - 3ms/step\n",
      "Epoch 66/200\n",
      "12/12 - 0s - loss: 931164258955123752960.0000 - mae: 1836628864.0000 - val_loss: 60376972475986897534976.0000 - val_mae: 21104414720.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 67/200\n",
      "12/12 - 0s - loss: 931105219578758692864.0000 - mae: 1841445120.0000 - val_loss: 60376094274059560288256.0000 - val_mae: 21108942848.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 68/200\n",
      "12/12 - 0s - loss: 931052583758113800192.0000 - mae: 1846253056.0000 - val_loss: 60375198057733713559552.0000 - val_mae: 21113499648.0000 - 41ms/epoch - 3ms/step\n",
      "Epoch 69/200\n",
      "12/12 - 0s - loss: 930996992450213445632.0000 - mae: 1854999424.0000 - val_loss: 60373374099884628508672.0000 - val_mae: 21123450880.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 70/200\n",
      "12/12 - 0s - loss: 930913183275897847808.0000 - mae: 1862228352.0000 - val_loss: 60372221178380021661696.0000 - val_mae: 21129754624.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 71/200\n",
      "12/12 - 0s - loss: 930848725506231107584.0000 - mae: 1868224768.0000 - val_loss: 60371288933257155969024.0000 - val_mae: 21134755840.0000 - 38ms/epoch - 3ms/step\n",
      "Epoch 72/200\n",
      "12/12 - 0s - loss: 930813400396653920256.0000 - mae: 1874732160.0000 - val_loss: 60370154026151058604032.0000 - val_mae: 21140885504.0000 - 49ms/epoch - 4ms/step\n",
      "Epoch 73/200\n",
      "12/12 - 0s - loss: 930748098202057048064.0000 - mae: 1879332224.0000 - val_loss: 60369352385417386655744.0000 - val_mae: 21144639488.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 74/200\n",
      "12/12 - 0s - loss: 930719669229409271808.0000 - mae: 1885406080.0000 - val_loss: 60368217478311289290752.0000 - val_mae: 21150941184.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 75/200\n",
      "12/12 - 0s - loss: 930637760011186470912.0000 - mae: 1889460352.0000 - val_loss: 60367623003160476385280.0000 - val_mae: 21154578432.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 76/200\n",
      "12/12 - 0s - loss: 930626430643373867008.0000 - mae: 1895129984.0000 - val_loss: 60366357491665185275904.0000 - val_mae: 21161558016.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 77/200\n",
      "12/12 - 0s - loss: 930543747368965111808.0000 - mae: 1899519872.0000 - val_loss: 60365812556110273445888.0000 - val_mae: 21165006848.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 78/200\n",
      "12/12 - 0s - loss: 930454801276324544512.0000 - mae: 1909478784.0000 - val_loss: 60363380612311493378048.0000 - val_mae: 21178699776.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 79/200\n",
      "12/12 - 0s - loss: 930367403296055885824.0000 - mae: 1919261440.0000 - val_loss: 60362065561220301193216.0000 - val_mae: 21185628160.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 80/200\n",
      "12/12 - 0s - loss: 930307097282295627776.0000 - mae: 1925110016.0000 - val_loss: 60361232395289237651456.0000 - val_mae: 21190449152.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 81/200\n",
      "12/12 - 0s - loss: 930267409310579425280.0000 - mae: 1930529408.0000 - val_loss: 60360336178963390922752.0000 - val_mae: 21195290624.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 82/200\n",
      "12/12 - 0s - loss: 930223991795421806592.0000 - mae: 1935194880.0000 - val_loss: 60359282336650586226688.0000 - val_mae: 21200783360.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 83/200\n",
      "12/12 - 0s - loss: 930186344517286756352.0000 - mae: 1941856384.0000 - val_loss: 60358133918745606750208.0000 - val_mae: 21207320576.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 84/200\n",
      "12/12 - 0s - loss: 930128219934596005888.0000 - mae: 1947635968.0000 - val_loss: 60357269227617151614976.0000 - val_mae: 21212663808.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 85/200\n",
      "12/12 - 0s - loss: 930097187318413656064.0000 - mae: 1954084352.0000 - val_loss: 60356098291714035286016.0000 - val_mae: 21218879488.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 86/200\n",
      "12/12 - 0s - loss: 930038570154513661952.0000 - mae: 1957590144.0000 - val_loss: 60355715485745708793856.0000 - val_mae: 21221138432.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 87/200\n",
      "12/12 - 0s - loss: 930013448512842235904.0000 - mae: 1963258368.0000 - val_loss: 60354368909457125015552.0000 - val_mae: 21228695552.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 88/200\n",
      "12/12 - 0s - loss: 929942868662432038912.0000 - mae: 1968620288.0000 - val_loss: 60353648333516745736192.0000 - val_mae: 21232898048.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 89/200\n",
      "12/12 - 0s - loss: 929903673271925080064.0000 - mae: 1973449728.0000 - val_loss: 60352864707181583269888.0000 - val_mae: 21237440512.0000 - 49ms/epoch - 4ms/step\n",
      "Epoch 90/200\n",
      "12/12 - 0s - loss: 929883336704857735168.0000 - mae: 1979896576.0000 - val_loss: 60351437066099706822656.0000 - val_mae: 21245585408.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 91/200\n",
      "12/12 - 0s - loss: 929819793728865304576.0000 - mae: 1986389376.0000 - val_loss: 60350329180591373680640.0000 - val_mae: 21251379200.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 92/200\n",
      "12/12 - 0s - loss: 929785383412962426880.0000 - mae: 1994075008.0000 - val_loss: 60349113208691983646720.0000 - val_mae: 21258600448.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 93/200\n",
      "12/12 - 0s - loss: 929712692500226899968.0000 - mae: 1998665088.0000 - val_loss: 60348464690345642295296.0000 - val_mae: 21262442496.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 94/200\n",
      "12/12 - 0s - loss: 929673989690929184768.0000 - mae: 2002040704.0000 - val_loss: 60347771136003027238912.0000 - val_mae: 21266429952.0000 - 42ms/epoch - 3ms/step\n",
      "Epoch 95/200\n",
      "12/12 - 0s - loss: 929641620068607459328.0000 - mae: 2009235840.0000 - val_loss: 60346550660504009834496.0000 - val_mae: 21275410432.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 96/200\n",
      "12/12 - 0s - loss: 929624027882563043328.0000 - mae: 2019624960.0000 - val_loss: 60345091494224741793792.0000 - val_mae: 21283717120.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 97/200\n",
      "12/12 - 0s - loss: 929542048295596064768.0000 - mae: 2025550336.0000 - val_loss: 60344154745502248730624.0000 - val_mae: 21287813120.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 98/200\n",
      "12/12 - 0s - loss: 929506723186018877440.0000 - mae: 2029019648.0000 - val_loss: 60343123421187580887040.0000 - val_mae: 21292943360.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 99/200\n",
      "12/12 - 0s - loss: 929454579946583228416.0000 - mae: 2034096768.0000 - val_loss: 60342132629269559377920.0000 - val_mae: 21298714624.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 100/200\n",
      "12/12 - 0s - loss: 929416440087238934528.0000 - mae: 2038938368.0000 - val_loss: 60341385031731415875584.0000 - val_mae: 21303654400.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 101/200\n",
      "12/12 - 0s - loss: 929373304047058026496.0000 - mae: 2045145600.0000 - val_loss: 60340304167820846956544.0000 - val_mae: 21309829120.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 102/200\n",
      "12/12 - 0s - loss: 929332279069202448384.0000 - mae: 2051368064.0000 - val_loss: 60339331390301334929408.0000 - val_mae: 21315360768.0000 - 48ms/epoch - 4ms/step\n",
      "Epoch 103/200\n",
      "12/12 - 0s - loss: 929295476215997530112.0000 - mae: 2055515904.0000 - val_loss: 60338723404351639912448.0000 - val_mae: 21318213632.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 104/200\n",
      "12/12 - 0s - loss: 929208500448193937408.0000 - mae: 2064632320.0000 - val_loss: 60336350007348015661056.0000 - val_mae: 21332424704.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 105/200\n",
      "12/12 - 0s - loss: 929150516602991542272.0000 - mae: 2075919104.0000 - val_loss: 60334899848268002361344.0000 - val_mae: 21341184000.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 106/200\n",
      "12/12 - 0s - loss: 929104636181787705344.0000 - mae: 2083268352.0000 - val_loss: 60333706394366749179904.0000 - val_mae: 21347811328.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 107/200\n",
      "12/12 - 0s - loss: 929087818051929243648.0000 - mae: 2092282880.0000 - val_loss: 60332238220888226398208.0000 - val_mae: 21356238848.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 108/200\n",
      "12/12 - 0s - loss: 929019771476309442560.0000 - mae: 2098535680.0000 - val_loss: 60331355515361261780992.0000 - val_mae: 21361104896.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 109/200\n",
      "12/12 - 0s - loss: 928989724022545580032.0000 - mae: 2102402944.0000 - val_loss: 60330589903424608796672.0000 - val_mae: 21365899264.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 110/200\n",
      "12/12 - 0s - loss: 928956861819014610944.0000 - mae: 2107754496.0000 - val_loss: 60329612622305469399040.0000 - val_mae: 21371633664.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 111/200\n",
      "12/12 - 0s - loss: 928912670247671037952.0000 - mae: 2113325952.0000 - val_loss: 60328910060763599601664.0000 - val_mae: 21375813632.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 112/200\n",
      "12/12 - 0s - loss: 928905070423299850240.0000 - mae: 2120302080.0000 - val_loss: 60327396851288803115008.0000 - val_mae: 21384140800.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 113/200\n",
      "12/12 - 0s - loss: 928842723715958439936.0000 - mae: 2126353408.0000 - val_loss: 60326644750151032242176.0000 - val_mae: 21388285952.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 114/200\n",
      "12/12 - 0s - loss: 928816405805635993600.0000 - mae: 2129792256.0000 - val_loss: 60326054778599846707200.0000 - val_mae: 21392099328.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 115/200\n",
      "12/12 - 0s - loss: 928795365551126872064.0000 - mae: 2136030720.0000 - val_loss: 60324811785102692450304.0000 - val_mae: 21399216128.0000 - 49ms/epoch - 4ms/step\n",
      "Epoch 116/200\n",
      "12/12 - 0s - loss: 928754340573271293952.0000 - mae: 2142652928.0000 - val_loss: 60323703899594359308288.0000 - val_mae: 21405323264.0000 - 51ms/epoch - 4ms/step\n",
      "Epoch 117/200\n",
      "12/12 - 0s - loss: 928735341012343324672.0000 - mae: 2149926144.0000 - val_loss: 60322587006886771425280.0000 - val_mae: 21411999744.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 118/200\n",
      "12/12 - 0s - loss: 928682845929186787328.0000 - mae: 2154449664.0000 - val_loss: 60322019553333722742784.0000 - val_mae: 21415477248.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 119/200\n",
      "12/12 - 0s - loss: 928677286798396751872.0000 - mae: 2159868416.0000 - val_loss: 60320929682223899082752.0000 - val_mae: 21422022656.0000 - 49ms/epoch - 4ms/step\n",
      "Epoch 120/200\n",
      "12/12 - 0s - loss: 928632813752076468224.0000 - mae: 2166108160.0000 - val_loss: 60319988429901778649088.0000 - val_mae: 21427755008.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 121/200\n",
      "12/12 - 0s - loss: 928598614542406123520.0000 - mae: 2170352896.0000 - val_loss: 60319362429553574150144.0000 - val_mae: 21430982656.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 122/200\n",
      "12/12 - 0s - loss: 928577715025385357312.0000 - mae: 2174271744.0000 - val_loss: 60318565292419529572352.0000 - val_mae: 21435942912.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 123/200\n",
      "12/12 - 0s - loss: 928547667571621494784.0000 - mae: 2178241792.0000 - val_loss: 60317840212879522922496.0000 - val_mae: 21439854592.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 124/200\n",
      "12/12 - 0s - loss: 928517831224090165248.0000 - mae: 2182981888.0000 - val_loss: 60317236730529455276032.0000 - val_mae: 21443727360.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 125/200\n",
      "12/12 - 0s - loss: 928495594700930023424.0000 - mae: 2187296512.0000 - val_loss: 60316444096995038068736.0000 - val_mae: 21447968768.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 126/200\n",
      "12/12 - 0s - loss: 928469980478049353728.0000 - mae: 2191723264.0000 - val_loss: 60315552384268818710528.0000 - val_mae: 21453295616.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 127/200\n",
      "12/12 - 0s - loss: 928462099178701455360.0000 - mae: 2199741696.0000 - val_loss: 60313931088402965331968.0000 - val_mae: 21462214656.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 128/200\n",
      "12/12 - 0s - loss: 928416289126241796096.0000 - mae: 2204664832.0000 - val_loss: 60313417678045445095424.0000 - val_mae: 21465456640.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 129/200\n",
      "12/12 - 0s - loss: 928395671084197740544.0000 - mae: 2211326464.0000 - val_loss: 60312512454520343625728.0000 - val_mae: 21471145984.0000 - 55ms/epoch - 5ms/step\n",
      "Epoch 130/200\n",
      "12/12 - 0s - loss: 928361753349504106496.0000 - mae: 2215220992.0000 - val_loss: 60311899964971021238272.0000 - val_mae: 21474494464.0000 - 92ms/epoch - 8ms/step\n",
      "Epoch 131/200\n",
      "12/12 - 0s - loss: 928338602032669655040.0000 - mae: 2218208000.0000 - val_loss: 60311161374632132476928.0000 - val_mae: 21478766592.0000 - 77ms/epoch - 6ms/step\n",
      "Epoch 132/200\n",
      "12/12 - 0s - loss: 928325091233787543552.0000 - mae: 2224542464.0000 - val_loss: 60309864337939449774080.0000 - val_mae: 21486233600.0000 - 58ms/epoch - 5ms/step\n",
      "Epoch 133/200\n",
      "12/12 - 0s - loss: 928300884385790427136.0000 - mae: 2232122624.0000 - val_loss: 60308796984827762966528.0000 - val_mae: 21492463616.0000 - 52ms/epoch - 4ms/step\n",
      "Epoch 134/200\n",
      "12/12 - 0s - loss: 928277381225235087360.0000 - mae: 2238131712.0000 - val_loss: 60307932293699307831296.0000 - val_mae: 21497688064.0000 - 55ms/epoch - 5ms/step\n",
      "Epoch 135/200\n",
      "12/12 - 0s - loss: 928248741146354778112.0000 - mae: 2240695552.0000 - val_loss: 60307526969732844486656.0000 - val_mae: 21500278784.0000 - 54ms/epoch - 5ms/step\n",
      "Epoch 136/200\n",
      "12/12 - 0s - loss: 928238185834728128512.0000 - mae: 2244951808.0000 - val_loss: 60307355832947004407808.0000 - val_mae: 21503272960.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 137/200\n",
      "12/12 - 0s - loss: 928212079030638215168.0000 - mae: 2249852928.0000 - val_loss: 60306022767457302740992.0000 - val_mae: 21509793792.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 138/200\n",
      "12/12 - 0s - loss: 928185479645339058176.0000 - mae: 2253508096.0000 - val_loss: 60305558896695683579904.0000 - val_mae: 21511680000.0000 - 52ms/epoch - 4ms/step\n",
      "Epoch 139/200\n",
      "12/12 - 0s - loss: 928171335527759347712.0000 - mae: 2255676928.0000 - val_loss: 60304955414345615933440.0000 - val_mae: 21514997760.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 140/200\n",
      "12/12 - 0s - loss: 928222634342264864768.0000 - mae: 2269496576.0000 - val_loss: 60301735340612046028800.0000 - val_mae: 21533679616.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 141/200\n",
      "12/12 - 0s - loss: 928080067266560917504.0000 - mae: 2278255872.0000 - val_loss: 60301217426654898421760.0000 - val_mae: 21536929792.0000 - 50ms/epoch - 4ms/step\n",
      "Epoch 142/200\n",
      "12/12 - 0s - loss: 928073663710840750080.0000 - mae: 2284965888.0000 - val_loss: 60300145569943584243712.0000 - val_mae: 21543174144.0000 - 49ms/epoch - 4ms/step\n",
      "Epoch 143/200\n",
      "12/12 - 0s - loss: 928044882894472085504.0000 - mae: 2289060096.0000 - val_loss: 60299456519200596557824.0000 - val_mae: 21547546624.0000 - 48ms/epoch - 4ms/step\n",
      "Epoch 144/200\n",
      "12/12 - 0s - loss: 928026375914753359872.0000 - mae: 2292527104.0000 - val_loss: 60298830518852392058880.0000 - val_mae: 21550704640.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 145/200\n",
      "12/12 - 0s - loss: 928019339040335593472.0000 - mae: 2299159552.0000 - val_loss: 60297911784528408477696.0000 - val_mae: 21558118400.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 146/200\n",
      "12/12 - 0s - loss: 927993935923687456768.0000 - mae: 2304558080.0000 - val_loss: 60297245251783557644288.0000 - val_mae: 21561239552.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 147/200\n",
      "12/12 - 0s - loss: 927980425124805345280.0000 - mae: 2308743680.0000 - val_loss: 60296673294630881591296.0000 - val_mae: 21565741056.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 148/200\n",
      "12/12 - 0s - loss: 927971910506759847936.0000 - mae: 2310248704.0000 - val_loss: 60296493150645786771456.0000 - val_mae: 21566201856.0000 - 56ms/epoch - 5ms/step\n",
      "Epoch 149/200\n",
      "12/12 - 0s - loss: 927959384870296223744.0000 - mae: 2311429888.0000 - val_loss: 60295700517111369564160.0000 - val_mae: 21569501184.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 150/200\n",
      "12/12 - 0s - loss: 927946507390111711232.0000 - mae: 2313816832.0000 - val_loss: 60295295193144906219520.0000 - val_mae: 21571770368.0000 - 42ms/epoch - 3ms/step\n",
      "Epoch 151/200\n",
      "12/12 - 0s - loss: 927928985572811472896.0000 - mae: 2317356032.0000 - val_loss: 60294700717994093314048.0000 - val_mae: 21575493632.0000 - 42ms/epoch - 3ms/step\n",
      "Epoch 152/200\n",
      "12/12 - 0s - loss: 927911745230487945216.0000 - mae: 2321354752.0000 - val_loss: 60293836026865638178816.0000 - val_mae: 21580308480.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 153/200\n",
      "12/12 - 0s - loss: 927883034782863458304.0000 - mae: 2332870144.0000 - val_loss: 60291178903085489586176.0000 - val_mae: 21596444672.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 154/200\n",
      "12/12 - 0s - loss: 927842220911240413184.0000 - mae: 2345111040.0000 - val_loss: 60290111549973802778624.0000 - val_mae: 21602541568.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 155/200\n",
      "12/12 - 0s - loss: 927828287899893235712.0000 - mae: 2350838528.0000 - val_loss: 60289246858845347643392.0000 - val_mae: 21607647232.0000 - 52ms/epoch - 4ms/step\n",
      "Epoch 156/200\n",
      "12/12 - 0s - loss: 927824769462684352512.0000 - mae: 2355332608.0000 - val_loss: 60288413692914284101632.0000 - val_mae: 21612226560.0000 - 69ms/epoch - 6ms/step\n",
      "Epoch 157/200\n",
      "12/12 - 0s - loss: 927807177276639936512.0000 - mae: 2358920192.0000 - val_loss: 60287967836551174422528.0000 - val_mae: 21615157248.0000 - 51ms/epoch - 4ms/step\n",
      "Epoch 158/200\n",
      "12/12 - 0s - loss: 927791766521665028096.0000 - mae: 2362212608.0000 - val_loss: 60287278785808186736640.0000 - val_mae: 21618589696.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 159/200\n",
      "12/12 - 0s - loss: 927793244265292759040.0000 - mae: 2363547136.0000 - val_loss: 60287355347001852035072.0000 - val_mae: 21619181568.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 160/200\n",
      "12/12 - 0s - loss: nan - mae: nan - val_loss: nan - val_mae: nan - 43ms/epoch - 4ms/step\n",
      "Epoch 161/200\n",
      "12/12 - 0s - loss: nan - mae: nan - val_loss: nan - val_mae: nan - 45ms/epoch - 4ms/step\n",
      "Epoch 162/200\n",
      "12/12 - 0s - loss: nan - mae: nan - val_loss: nan - val_mae: nan - 44ms/epoch - 4ms/step\n",
      "Epoch 163/200\n",
      "12/12 - 0s - loss: nan - mae: nan - val_loss: nan - val_mae: nan - 44ms/epoch - 4ms/step\n",
      "Epoch 164/200\n",
      "12/12 - 0s - loss: nan - mae: nan - val_loss: nan - val_mae: nan - 43ms/epoch - 4ms/step\n",
      "Epoch 165/200\n",
      "12/12 - 0s - loss: nan - mae: nan - val_loss: nan - val_mae: nan - 46ms/epoch - 4ms/step\n",
      "Epoch 166/200\n",
      "12/12 - 0s - loss: nan - mae: nan - val_loss: nan - val_mae: nan - 43ms/epoch - 4ms/step\n",
      "Epoch 167/200\n",
      "12/12 - 0s - loss: nan - mae: nan - val_loss: nan - val_mae: nan - 42ms/epoch - 4ms/step\n",
      "Epoch 168/200\n",
      "12/12 - 0s - loss: nan - mae: nan - val_loss: nan - val_mae: nan - 45ms/epoch - 4ms/step\n"
     ]
    }
   ],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "xTrain = xTrain.astype(np.float32)\n",
    "xVal = xVal.astype(np.float32)\n",
    "yTrain = yTrain.astype(np.float32)\n",
    "yVal = yVal.astype(np.float32)\n",
    "\n",
    "history = model.fit(\n",
    "    xTrain, yTrain,\n",
    "    validation_data=(xVal, yVal),\n",
    "    epochs=200,\n",
    "    batch_size=16,\n",
    "    callbacks=[es],\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a32cf2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('flightPathModel.h5')\n",
    "\n",
    "import json\n",
    "with open('columns.json', 'w') as f:\n",
    "    json.dump(list(x.columns), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
