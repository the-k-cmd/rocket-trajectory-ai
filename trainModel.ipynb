{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3437961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b667345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('LaunchData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "96f07167",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['RocketName'])\n",
    "x = df.drop(columns=['AltitudeHeight', 'TimeAtAltitude', 'TotalFlightTime'])\n",
    "y = df[['AltitudeHeight', 'TimeAtAltitude', 'TotalFlightTime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f1a327da",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.get_dummies(x, columns=['NoseType', 'FinShape'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "eccf5348",
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xVal, yTrain, yVal = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fc6e5457",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(xTrain.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(3)  \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "24cc39a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "12/12 - 1s - loss: 932994338884952260608.0000 - mae: 1684693504.0000 - val_loss: 60406894391911147110400.0000 - val_mae: 20984770560.0000 - 599ms/epoch - 50ms/step\n",
      "Epoch 2/200\n",
      "12/12 - 0s - loss: 932991735241417687040.0000 - mae: 1684546432.0000 - val_loss: 60406871873913010257920.0000 - val_mae: 20984690688.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 3/200\n",
      "12/12 - 0s - loss: 932990116760301600768.0000 - mae: 1684441984.0000 - val_loss: 60406840348715618664448.0000 - val_mae: 20984668160.0000 - 49ms/epoch - 4ms/step\n",
      "Epoch 4/200\n",
      "12/12 - 0s - loss: 932987583485511204864.0000 - mae: 1684445952.0000 - val_loss: 60406813327117854441472.0000 - val_mae: 20984686592.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 5/200\n",
      "12/12 - 0s - loss: 932986316848116006912.0000 - mae: 1684545536.0000 - val_loss: 60406772794721208107008.0000 - val_mae: 20984825856.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 6/200\n",
      "12/12 - 0s - loss: 932983713204581433344.0000 - mae: 1684675840.0000 - val_loss: 60406750276723071254528.0000 - val_mae: 20984922112.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 7/200\n",
      "12/12 - 0s - loss: 932981531773511925760.0000 - mae: 1684745088.0000 - val_loss: 60406718751525679661056.0000 - val_mae: 20984938496.0000 - 74ms/epoch - 6ms/step\n",
      "Epoch 8/200\n",
      "12/12 - 0s - loss: 932980194767372550144.0000 - mae: 1684812032.0000 - val_loss: 60406682722728660697088.0000 - val_mae: 20985038848.0000 - 81ms/epoch - 7ms/step\n",
      "Epoch 9/200\n",
      "12/12 - 0s - loss: 932977731861326331904.0000 - mae: 1684909568.0000 - val_loss: 60406651197531269103616.0000 - val_mae: 20985120768.0000 - 80ms/epoch - 7ms/step\n",
      "Epoch 10/200\n",
      "12/12 - 0s - loss: 932975339324024291328.0000 - mae: 1684991872.0000 - val_loss: 60406619672333877510144.0000 - val_mae: 20985186304.0000 - 68ms/epoch - 6ms/step\n",
      "Epoch 11/200\n",
      "12/12 - 0s - loss: 932972735680489717760.0000 - mae: 1685041920.0000 - val_loss: 60406583643536858546176.0000 - val_mae: 20985196544.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 12/200\n",
      "12/12 - 0s - loss: 932970694986908565504.0000 - mae: 1685109376.0000 - val_loss: 60406543111140212211712.0000 - val_mae: 20985274368.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 13/200\n",
      "12/12 - 0s - loss: 932967387655932215296.0000 - mae: 1685213824.0000 - val_loss: 60406489067944683765760.0000 - val_mae: 20985372672.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 14/200\n",
      "12/12 - 0s - loss: 932963587743746621440.0000 - mae: 1685323392.0000 - val_loss: 60406457542747292172288.0000 - val_mae: 20985425920.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 15/200\n",
      "12/12 - 0s - loss: 932961124837700403200.0000 - mae: 1685463424.0000 - val_loss: 60406389988752881614848.0000 - val_mae: 20985573376.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 16/200\n",
      "12/12 - 0s - loss: 932958098981700763648.0000 - mae: 1685668224.0000 - val_loss: 60406317931158843686912.0000 - val_mae: 20985710592.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 17/200\n",
      "12/12 - 0s - loss: 932952258375934017536.0000 - mae: 1685781120.0000 - val_loss: 60406286405961452093440.0000 - val_mae: 20985763840.0000 - 41ms/epoch - 3ms/step\n",
      "Epoch 18/200\n",
      "12/12 - 0s - loss: 932947191826353225728.0000 - mae: 1686099840.0000 - val_loss: 60406182823170022572032.0000 - val_mae: 20986093568.0000 - 41ms/epoch - 3ms/step\n",
      "Epoch 19/200\n",
      "12/12 - 0s - loss: 932942054908028256256.0000 - mae: 1686443520.0000 - val_loss: 60406106261976357273600.0000 - val_mae: 20986271744.0000 - 41ms/epoch - 3ms/step\n",
      "Epoch 20/200\n",
      "12/12 - 0s - loss: 932937762414633418752.0000 - mae: 1686711680.0000 - val_loss: 60406020693583437234176.0000 - val_mae: 20986427392.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 21/200\n",
      "12/12 - 0s - loss: 932930866277704007680.0000 - mae: 1686914304.0000 - val_loss: 60405966650387908788224.0000 - val_mae: 20986521600.0000 - 40ms/epoch - 3ms/step\n",
      "Epoch 22/200\n",
      "12/12 - 0s - loss: 932927488577983479808.0000 - mae: 1687344256.0000 - val_loss: 60405818031600205561856.0000 - val_mae: 20986896384.0000 - 40ms/epoch - 3ms/step\n",
      "Epoch 23/200\n",
      "12/12 - 0s - loss: 932918340641240383488.0000 - mae: 1687856512.0000 - val_loss: 60405718952408403410944.0000 - val_mae: 20987209728.0000 - 40ms/epoch - 3ms/step\n",
      "Epoch 24/200\n",
      "12/12 - 0s - loss: 932909544548218175488.0000 - mae: 1688260480.0000 - val_loss: 60405642391214738112512.0000 - val_mae: 20987422720.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 25/200\n",
      "12/12 - 0s - loss: 932899622555289124864.0000 - mae: 1688901120.0000 - val_loss: 60405466750829270663168.0000 - val_mae: 20988028928.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 26/200\n",
      "12/12 - 0s - loss: 932891107937243627520.0000 - mae: 1689692160.0000 - val_loss: 60405340650039704289280.0000 - val_mae: 20988372992.0000 - 41ms/epoch - 3ms/step\n",
      "Epoch 27/200\n",
      "12/12 - 0s - loss: 932885548806453592064.0000 - mae: 1690228096.0000 - val_loss: 60405219052849765285888.0000 - val_mae: 20988727296.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 28/200\n",
      "12/12 - 0s - loss: 932875345338547830784.0000 - mae: 1690730112.0000 - val_loss: 60405128980857217875968.0000 - val_mae: 20989001728.0000 - 41ms/epoch - 3ms/step\n",
      "Epoch 29/200\n",
      "12/12 - 0s - loss: 932868026989153353728.0000 - mae: 1691746432.0000 - val_loss: 60404885786477339869184.0000 - val_mae: 20989937664.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 30/200\n",
      "12/12 - 0s - loss: 932852194021713379328.0000 - mae: 1692641920.0000 - val_loss: 60404755182088146124800.0000 - val_mae: 20990271488.0000 - 41ms/epoch - 3ms/step\n",
      "Epoch 31/200\n",
      "12/12 - 0s - loss: 932842975716226105344.0000 - mae: 1693178624.0000 - val_loss: 60404651599296716603392.0000 - val_mae: 20990564352.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 32/200\n",
      "12/12 - 0s - loss: 932838612854087090176.0000 - mae: 1694392832.0000 - val_loss: 60404372376119819632640.0000 - val_mae: 20991666176.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 33/200\n",
      "12/12 - 0s - loss: 932817502230833790976.0000 - mae: 1695299328.0000 - val_loss: 60404228260931743776768.0000 - val_mae: 20992008192.0000 - 42ms/epoch - 3ms/step\n",
      "Epoch 34/200\n",
      "12/12 - 0s - loss: 932808495031579049984.0000 - mae: 1695981952.0000 - val_loss: 60404084145743667920896.0000 - val_mae: 20992423936.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 35/200\n",
      "12/12 - 0s - loss: 932799347094835953664.0000 - mae: 1696614656.0000 - val_loss: 60403940030555592065024.0000 - val_mae: 20992835584.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 36/200\n",
      "12/12 - 0s - loss: 932790128789348679680.0000 - mae: 1697398912.0000 - val_loss: 60403786908168261468160.0000 - val_mae: 20993345536.0000 - 41ms/epoch - 3ms/step\n",
      "Epoch 37/200\n",
      "12/12 - 0s - loss: 932778940159024431104.0000 - mae: 1698079488.0000 - val_loss: 60403638289380558241792.0000 - val_mae: 20993806336.0000 - 58ms/epoch - 5ms/step\n",
      "Epoch 38/200\n",
      "12/12 - 0s - loss: 932767821897444360192.0000 - mae: 1698828032.0000 - val_loss: 60403494174192482385920.0000 - val_mae: 20994269184.0000 - 63ms/epoch - 5ms/step\n",
      "Epoch 39/200\n",
      "12/12 - 0s - loss: 932758744329445441536.0000 - mae: 1699818112.0000 - val_loss: 60403287008609623343104.0000 - val_mae: 20994947072.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 40/200\n",
      "12/12 - 0s - loss: 932744107630656487424.0000 - mae: 1700585984.0000 - val_loss: 60403111368224155893760.0000 - val_mae: 20995340288.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 41/200\n",
      "12/12 - 0s - loss: 932732778262843883520.0000 - mae: 1701430016.0000 - val_loss: 60402899699041669480448.0000 - val_mae: 20995977216.0000 - 50ms/epoch - 4ms/step\n",
      "Epoch 42/200\n",
      "12/12 - 0s - loss: 932716593451683020800.0000 - mae: 1702336000.0000 - val_loss: 60402746576654338883584.0000 - val_mae: 20996386816.0000 - 42ms/epoch - 3ms/step\n",
      "Epoch 43/200\n",
      "12/12 - 0s - loss: 932704278921451929600.0000 - mae: 1703428480.0000 - val_loss: 60402498878674833506304.0000 - val_mae: 20997292032.0000 - 40ms/epoch - 3ms/step\n",
      "Epoch 44/200\n",
      "12/12 - 0s - loss: 932690768122569818112.0000 - mae: 1704852224.0000 - val_loss: 60402197137499799683072.0000 - val_mae: 20998221824.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 45/200\n",
      "12/12 - 0s - loss: 932670079711781584896.0000 - mae: 1706120704.0000 - val_loss: 60401976461118058528768.0000 - val_mae: 20998885376.0000 - 42ms/epoch - 3ms/step\n",
      "Epoch 46/200\n",
      "12/12 - 0s - loss: 932657131862852894720.0000 - mae: 1707737472.0000 - val_loss: 60401656705544515223552.0000 - val_mae: 20999952384.0000 - 40ms/epoch - 3ms/step\n",
      "Epoch 47/200\n",
      "12/12 - 0s - loss: 932636865664529727488.0000 - mae: 1709169280.0000 - val_loss: 60401363971568736141312.0000 - val_mae: 21000830976.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 48/200\n",
      "12/12 - 0s - loss: 932613925453927809024.0000 - mae: 1710317696.0000 - val_loss: 60401183827583641321472.0000 - val_mae: 21001363456.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 49/200\n",
      "12/12 - 0s - loss: 932604707148440535040.0000 - mae: 1712081152.0000 - val_loss: 60400760489218668494848.0000 - val_mae: 21002811392.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 50/200\n",
      "12/12 - 0s - loss: 932575081907141738496.0000 - mae: 1713505408.0000 - val_loss: 60400467755242889412608.0000 - val_mae: 21003495424.0000 - 53ms/epoch - 4ms/step\n",
      "Epoch 51/200\n",
      "12/12 - 0s - loss: 932554463865097682944.0000 - mae: 1715222272.0000 - val_loss: 60400111970872327143424.0000 - val_mae: 21004802048.0000 - 50ms/epoch - 4ms/step\n",
      "Epoch 52/200\n",
      "12/12 - 0s - loss: 932531453285751586816.0000 - mae: 1717340800.0000 - val_loss: 60399733668503628021760.0000 - val_mae: 21005910016.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 53/200\n",
      "12/12 - 0s - loss: 932512031512358551552.0000 - mae: 1719368960.0000 - val_loss: 60399296819339773083648.0000 - val_mae: 21007505408.0000 - 42ms/epoch - 3ms/step\n",
      "Epoch 54/200\n",
      "12/12 - 0s - loss: 932483813645943308288.0000 - mae: 1721675520.0000 - val_loss: 60398868977375172886528.0000 - val_mae: 21009020928.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 55/200\n",
      "12/12 - 0s - loss: 932449192223807897600.0000 - mae: 1723274368.0000 - val_loss: 60398531207403120099328.0000 - val_mae: 21010192384.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 56/200\n",
      "12/12 - 0s - loss: 932414359695439953920.0000 - mae: 1726223872.0000 - val_loss: 60397815135062368190464.0000 - val_mae: 21012711424.0000 - 42ms/epoch - 3ms/step\n",
      "Epoch 57/200\n",
      "12/12 - 0s - loss: 932379527167072010240.0000 - mae: 1729435008.0000 - val_loss: 60397265695907828989952.0000 - val_mae: 21014638592.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 58/200\n",
      "12/12 - 0s - loss: 932340824357774295040.0000 - mae: 1731574144.0000 - val_loss: 60396932429535403573248.0000 - val_mae: 21015717888.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 59/200\n",
      "12/12 - 0s - loss: 932316687878521356288.0000 - mae: 1733866240.0000 - val_loss: 60396396501179746484224.0000 - val_mae: 21017669632.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 60/200\n",
      "12/12 - 0s - loss: 932285162681129762816.0000 - mae: 1736816384.0000 - val_loss: 60395842558425579913216.0000 - val_mae: 21019537408.0000 - 41ms/epoch - 3ms/step\n",
      "Epoch 61/200\n",
      "12/12 - 0s - loss: 932251244946436128768.0000 - mae: 1739672064.0000 - val_loss: 60395311133669550194688.0000 - val_mae: 21021464576.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 62/200\n",
      "12/12 - 0s - loss: 932219579011556179968.0000 - mae: 1742351232.0000 - val_loss: 60394748183716128882688.0000 - val_mae: 21023459328.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 63/200\n",
      "12/12 - 0s - loss: 932180876202258464768.0000 - mae: 1744890368.0000 - val_loss: 60394284312954509721600.0000 - val_mae: 21024980992.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 64/200\n",
      "12/12 - 0s - loss: 932146747361332297728.0000 - mae: 1748123136.0000 - val_loss: 60393743880999225262080.0000 - val_mae: 21026938880.0000 - 41ms/epoch - 3ms/step\n",
      "Epoch 65/200\n",
      "12/12 - 0s - loss: 932128240381613572096.0000 - mae: 1751695232.0000 - val_loss: 60392955751064435425280.0000 - val_mae: 21029591040.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 66/200\n",
      "12/12 - 0s - loss: 932072297229992329216.0000 - mae: 1755072128.0000 - val_loss: 60392352268714367778816.0000 - val_mae: 21031737344.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 67/200\n",
      "12/12 - 0s - loss: 932030779670927507456.0000 - mae: 1757917056.0000 - val_loss: 60391789318760946466816.0000 - val_mae: 21033924608.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 68/200\n",
      "12/12 - 0s - loss: 931999254473535913984.0000 - mae: 1761367424.0000 - val_loss: 60391073246420194557952.0000 - val_mae: 21036505088.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 69/200\n",
      "12/12 - 0s - loss: 931951474096239280128.0000 - mae: 1764580864.0000 - val_loss: 60390429231673480577024.0000 - val_mae: 21039345664.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 70/200\n",
      "12/12 - 0s - loss: 931911223174569656320.0000 - mae: 1768757120.0000 - val_loss: 60389713159332728668160.0000 - val_mae: 21042352128.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 71/200\n",
      "12/12 - 0s - loss: 931863020584807956480.0000 - mae: 1771318784.0000 - val_loss: 60389258295770364248064.0000 - val_mae: 21044322304.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 72/200\n",
      "12/12 - 0s - loss: 931839165580531728384.0000 - mae: 1775364096.0000 - val_loss: 60388312539848616443904.0000 - val_mae: 21048621056.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 73/200\n",
      "12/12 - 0s - loss: 931780337310399201280.0000 - mae: 1780341376.0000 - val_loss: 60387551431511590830080.0000 - val_mae: 21052428288.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 74/200\n",
      "12/12 - 0s - loss: 931731431033195724800.0000 - mae: 1783422976.0000 - val_loss: 60386875891567485255680.0000 - val_mae: 21055686656.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 75/200\n",
      "12/12 - 0s - loss: 931700257679525019648.0000 - mae: 1788524544.0000 - val_loss: 60385934639245364822016.0000 - val_mae: 21060024320.0000 - 42ms/epoch - 3ms/step\n",
      "Epoch 76/200\n",
      "12/12 - 0s - loss: 931639529453299695616.0000 - mae: 1792307712.0000 - val_loss: 60385227574103867654144.0000 - val_mae: 21063354368.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 77/200\n",
      "12/12 - 0s - loss: 931594704563258523648.0000 - mae: 1796027136.0000 - val_loss: 60384412422571313594368.0000 - val_mae: 21067261952.0000 - 42ms/epoch - 3ms/step\n",
      "Epoch 78/200\n",
      "12/12 - 0s - loss: 931541576161404387328.0000 - mae: 1800680448.0000 - val_loss: 60383610781837641646080.0000 - val_mae: 21071204352.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 79/200\n",
      "12/12 - 0s - loss: 931496610533874860032.0000 - mae: 1805446912.0000 - val_loss: 60382696551113285435392.0000 - val_mae: 21075611648.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 80/200\n",
      "12/12 - 0s - loss: 931443482132020723712.0000 - mae: 1810282368.0000 - val_loss: 60381809341986693447680.0000 - val_mae: 21080205312.0000 - 42ms/epoch - 3ms/step\n",
      "Epoch 81/200\n",
      "12/12 - 0s - loss: 931390353730166587392.0000 - mae: 1815454464.0000 - val_loss: 60380931140059356200960.0000 - val_mae: 21084241920.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 82/200\n",
      "12/12 - 0s - loss: 931339125284405248000.0000 - mae: 1820256384.0000 - val_loss: 60380003398536117878784.0000 - val_mae: 21088471040.0000 - 41ms/epoch - 3ms/step\n",
      "Epoch 83/200\n",
      "12/12 - 0s - loss: 931297185512875360256.0000 - mae: 1825671680.0000 - val_loss: 60378918031025921589248.0000 - val_mae: 21093967872.0000 - 48ms/epoch - 4ms/step\n",
      "Epoch 84/200\n",
      "12/12 - 0s - loss: 931224283493907300352.0000 - mae: 1830051328.0000 - val_loss: 60378192951485914939392.0000 - val_mae: 21097996288.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 85/200\n",
      "12/12 - 0s - loss: 931186565847028072448.0000 - mae: 1836224384.0000 - val_loss: 60377107583975718649856.0000 - val_mae: 21103716352.0000 - 47ms/epoch - 4ms/step\n",
      "Epoch 86/200\n",
      "12/12 - 0s - loss: 931120489596245245952.0000 - mae: 1840365056.0000 - val_loss: 60376323957640556183552.0000 - val_mae: 21107410944.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 87/200\n",
      "12/12 - 0s - loss: 931076157287413317632.0000 - mae: 1846212096.0000 - val_loss: 60375283626126633598976.0000 - val_mae: 21114095616.0000 - 42ms/epoch - 3ms/step\n",
      "Epoch 88/200\n",
      "12/12 - 0s - loss: 931024717735419445248.0000 - mae: 1852940416.0000 - val_loss: 60374220776614574161920.0000 - val_mae: 21119475712.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 89/200\n",
      "12/12 - 0s - loss: 930968000527612248064.0000 - mae: 1857341568.0000 - val_loss: 60373293035091335839744.0000 - val_mae: 21123944448.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 90/200\n",
      "12/12 - 0s - loss: 930904457551619817472.0000 - mae: 1861198336.0000 - val_loss: 60372594977149093412864.0000 - val_mae: 21127825408.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 91/200\n",
      "12/12 - 0s - loss: 930822407595908661248.0000 - mae: 1869687296.0000 - val_loss: 60370748501301871509504.0000 - val_mae: 21138374656.0000 - 42ms/epoch - 3ms/step\n",
      "Epoch 92/200\n",
      "12/12 - 0s - loss: 930765760756845641728.0000 - mae: 1879085824.0000 - val_loss: 60369325363819622432768.0000 - val_mae: 21145925632.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 93/200\n",
      "12/12 - 0s - loss: 930689903250622119936.0000 - mae: 1885566720.0000 - val_loss: 60368334571901600923648.0000 - val_mae: 21151238144.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 94/200\n",
      "12/12 - 0s - loss: 930663585340299673600.0000 - mae: 1891839360.0000 - val_loss: 60367078067605564555264.0000 - val_mae: 21157588992.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 95/200\n",
      "12/12 - 0s - loss: 930577524366170390528.0000 - mae: 1896772736.0000 - val_loss: 60366393520462204239872.0000 - val_mae: 21161347072.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 96/200\n",
      "12/12 - 0s - loss: 930563098773613969408.0000 - mae: 1902136704.0000 - val_loss: 60365209073760205799424.0000 - val_mae: 21168027648.0000 - 41ms/epoch - 3ms/step\n",
      "Epoch 97/200\n",
      "12/12 - 0s - loss: 930492237448227061760.0000 - mae: 1907227008.0000 - val_loss: 60364276828637340106752.0000 - val_mae: 21172934656.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 98/200\n",
      "12/12 - 0s - loss: 930450297676697174016.0000 - mae: 1913747712.0000 - val_loss: 60363191461127143817216.0000 - val_mae: 21179258880.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 99/200\n",
      "12/12 - 0s - loss: 930388162075588296704.0000 - mae: 1918244736.0000 - val_loss: 60362425849190490832896.0000 - val_mae: 21183651840.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 100/200\n",
      "12/12 - 0s - loss: 930349600003778936832.0000 - mae: 1924152960.0000 - val_loss: 60361304452883275579392.0000 - val_mae: 21189627904.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 101/200\n",
      "12/12 - 0s - loss: 930290068046204633088.0000 - mae: 1928408448.0000 - val_loss: 60360890121717557493760.0000 - val_mae: 21192253440.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 102/200\n",
      "12/12 - 0s - loss: 930248831962116521984.0000 - mae: 1931912448.0000 - val_loss: 60359926351397300207616.0000 - val_mae: 21197191168.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 103/200\n",
      "12/12 - 0s - loss: 930199221997471268864.0000 - mae: 1937422208.0000 - val_loss: 60358962581077042921472.0000 - val_mae: 21202786304.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 104/200\n",
      "12/12 - 0s - loss: 930154537844918452224.0000 - mae: 1943610624.0000 - val_loss: 60357791645173926592512.0000 - val_mae: 21209282560.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 105/200\n",
      "12/12 - 0s - loss: 930120409003992285184.0000 - mae: 1951093760.0000 - val_loss: 60356467586883479666688.0000 - val_mae: 21216841728.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 106/200\n",
      "12/12 - 0s - loss: 930053277222046793728.0000 - mae: 1955675264.0000 - val_loss: 60355909140529685725184.0000 - val_mae: 21220087808.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 107/200\n",
      "12/12 - 0s - loss: 930013026300377169920.0000 - mae: 1962244992.0000 - val_loss: 60354787744222470471680.0000 - val_mae: 21227333632.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 108/200\n",
      "12/12 - 0s - loss: 929957927573686059008.0000 - mae: 1968345216.0000 - val_loss: 60353923053094015336448.0000 - val_mae: 21232023552.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 109/200\n",
      "12/12 - 0s - loss: 929931257819642724352.0000 - mae: 1973947648.0000 - val_loss: 60352689066796115820544.0000 - val_mae: 21238403072.0000 - 42ms/epoch - 3ms/step\n",
      "Epoch 110/200\n",
      "12/12 - 0s - loss: 929866940787464339456.0000 - mae: 1979425280.0000 - val_loss: 60351729800075485904896.0000 - val_mae: 21243596800.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 111/200\n",
      "12/12 - 0s - loss: 929833797108956659712.0000 - mae: 1985674368.0000 - val_loss: 60350558864172369575936.0000 - val_mae: 21250142208.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 112/200\n",
      "12/12 - 0s - loss: 929776939163661107200.0000 - mae: 1991667968.0000 - val_loss: 60349631122649131253760.0000 - val_mae: 21256013824.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 113/200\n",
      "12/12 - 0s - loss: 929752943421896523776.0000 - mae: 1998530176.0000 - val_loss: 60348361107554212773888.0000 - val_mae: 21262776320.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 114/200\n",
      "12/12 - 0s - loss: 929687711596043829248.0000 - mae: 2003774720.0000 - val_loss: 60347509927224639750144.0000 - val_mae: 21267755008.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 115/200\n",
      "12/12 - 0s - loss: 929662941798093291520.0000 - mae: 2010307840.0000 - val_loss: 60346366012919287644160.0000 - val_mae: 21274353664.0000 - 42ms/epoch - 4ms/step\n",
      "Epoch 116/200\n",
      "12/12 - 0s - loss: 929595035959961845760.0000 - mae: 2015071232.0000 - val_loss: 60345573379384870436864.0000 - val_mae: 21279078400.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 117/200\n",
      "12/12 - 0s - loss: 929558162738012749824.0000 - mae: 2019806592.0000 - val_loss: 60344690673857905819648.0000 - val_mae: 21284048896.0000 - 46ms/epoch - 4ms/step\n",
      "Epoch 118/200\n",
      "12/12 - 0s - loss: 929526356065644445696.0000 - mae: 2025782528.0000 - val_loss: 60343560270351435825152.0000 - val_mae: 21290442752.0000 - 43ms/epoch - 4ms/step\n",
      "Epoch 119/200\n",
      "12/12 - 0s - loss: 929472523976348532736.0000 - mae: 2031885056.0000 - val_loss: 60342677564824471207936.0000 - val_mae: 21295683584.0000 - 42ms/epoch - 3ms/step\n",
      "Epoch 120/200\n",
      "12/12 - 0s - loss: 929432554529655619584.0000 - mae: 2037289216.0000 - val_loss: 60341704787304959180800.0000 - val_mae: 21301532672.0000 - 40ms/epoch - 3ms/step\n",
      "Epoch 121/200\n",
      "12/12 - 0s - loss: 929389770333195599872.0000 - mae: 2041726336.0000 - val_loss: 60341087794156009422848.0000 - val_mae: 21305239552.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 122/200\n",
      "12/12 - 0s - loss: 929400044169845538816.0000 - mae: 2050430464.0000 - val_loss: 60339480009089038155776.0000 - val_mae: 21314478080.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 123/200\n",
      "12/12 - 0s - loss: 929310605495995727872.0000 - mae: 2056526720.0000 - val_loss: 60338552267565799833600.0000 - val_mae: 21319692288.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 124/200\n",
      "12/12 - 0s - loss: 929270073099349393408.0000 - mae: 2059890560.0000 - val_loss: 60338029850009024856064.0000 - val_mae: 21322780672.0000 - 42ms/epoch - 3ms/step\n",
      "Epoch 125/200\n",
      "12/12 - 0s - loss: 929249806901026226176.0000 - mae: 2063695360.0000 - val_loss: 60337061576089140199424.0000 - val_mae: 21328037888.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 126/200\n",
      "12/12 - 0s - loss: 929204419061031632896.0000 - mae: 2069084544.0000 - val_loss: 60336304971351741956096.0000 - val_mae: 21332563968.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 127/200\n",
      "12/12 - 0s - loss: 929166771782896582656.0000 - mae: 2073459968.0000 - val_loss: 60335669963804282716160.0000 - val_mae: 21336211456.0000 - 42ms/epoch - 3ms/step\n",
      "Epoch 128/200\n",
      "12/12 - 0s - loss: 929149883284293943296.0000 - mae: 2080031872.0000 - val_loss: 60334206293925387304960.0000 - val_mae: 21344581632.0000 - 44ms/epoch - 4ms/step\n",
      "Epoch 129/200\n",
      "12/12 - 0s - loss: 929107802775275700224.0000 - mae: 2088213248.0000 - val_loss: 60332958796828605677568.0000 - val_mae: 21351743488.0000 - 42ms/epoch - 3ms/step\n",
      "Epoch 130/200\n",
      "12/12 - 0s - loss: 929047567130259619840.0000 - mae: 2092348160.0000 - val_loss: 60332508436865868627968.0000 - val_mae: 21354948608.0000 - 45ms/epoch - 4ms/step\n",
      "Epoch 131/200\n",
      "12/12 - 0s - loss: nan - mae: nan - val_loss: nan - val_mae: nan - 42ms/epoch - 3ms/step\n",
      "Epoch 132/200\n",
      "12/12 - 0s - loss: nan - mae: nan - val_loss: nan - val_mae: nan - 47ms/epoch - 4ms/step\n",
      "Epoch 133/200\n",
      "12/12 - 0s - loss: nan - mae: nan - val_loss: nan - val_mae: nan - 42ms/epoch - 3ms/step\n",
      "Epoch 134/200\n",
      "12/12 - 0s - loss: nan - mae: nan - val_loss: nan - val_mae: nan - 41ms/epoch - 3ms/step\n",
      "Epoch 135/200\n",
      "12/12 - 0s - loss: nan - mae: nan - val_loss: nan - val_mae: nan - 44ms/epoch - 4ms/step\n",
      "Epoch 136/200\n",
      "12/12 - 0s - loss: nan - mae: nan - val_loss: nan - val_mae: nan - 43ms/epoch - 4ms/step\n",
      "Epoch 137/200\n",
      "12/12 - 0s - loss: nan - mae: nan - val_loss: nan - val_mae: nan - 41ms/epoch - 3ms/step\n",
      "Epoch 138/200\n",
      "12/12 - 0s - loss: nan - mae: nan - val_loss: nan - val_mae: nan - 41ms/epoch - 3ms/step\n",
      "Epoch 139/200\n",
      "12/12 - 0s - loss: nan - mae: nan - val_loss: nan - val_mae: nan - 45ms/epoch - 4ms/step\n",
      "Epoch 140/200\n",
      "12/12 - 0s - loss: nan - mae: nan - val_loss: nan - val_mae: nan - 44ms/epoch - 4ms/step\n"
     ]
    }
   ],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "xTrain = xTrain.astype(np.float32)\n",
    "xVal = xVal.astype(np.float32)\n",
    "yTrain = yTrain.astype(np.float32)\n",
    "yVal = yVal.astype(np.float32)\n",
    "\n",
    "history = model.fit(\n",
    "    xTrain, yTrain,\n",
    "    validation_data=(xVal, yVal),\n",
    "    epochs=200,\n",
    "    batch_size=16,\n",
    "    callbacks=[es],\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a32cf2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: flightPathModel\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('flightPathModel')\n",
    "import json\n",
    "with open('flightPathModel/columns.json', 'w') as f:\n",
    "    json.dump(list(x.columns), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2445bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
